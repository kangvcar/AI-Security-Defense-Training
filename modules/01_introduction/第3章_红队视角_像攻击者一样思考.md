# 第 3 章：红队视角——像攻击者一样思考

**章节目标**
- 理解红队测试的核心理念与职业定位
- 掌握攻击者的思维方式与行为模式
- 了解 CIA 三元组与 STRIDE for AI 威胁建模方法
- 明确 AI 安全测试的法律边界与伦理红线
- 建立负责任的安全研究态度

---

## 1. 红队是什么：安全的"压力测试员"（占比 20%）

### 1.1 红队与蓝队：网络安全的攻防对抗

**红队（Red Team）** 和 **蓝队（Blue Team）** 是网络安全领域的经典分工：

| 角色 | 职责 | 类比 | 目标 |
|------|------|------|------|
| **红队** | 模拟攻击者，主动寻找漏洞 | 小偷来"踩点"，测试门锁是否牢固 | 发现弱点，提供改进建议 |
| **蓝队** | 防御者，保护系统安全 | 保安巡逻，安装监控设备 | 检测攻击，快速响应 |
| **紫队** | 红蓝协作，攻防演练 | 保安和小偷一起讨论如何改进安全 | 提升整体安全水平 |

### 1.2 AI 红队的独特使命

在 AI 安全领域，红队的工作更具挑战性：

**传统软件红队**：
- 目标明确：找到代码漏洞、配置错误
- 工具成熟：有大量自动化扫描工具
- 修复清晰：打补丁、修改代码

**AI 红队**：
- 目标模糊：模型的"漏洞"可能是设计固有的
- 工具缺乏：AI 安全测试工具还在发展初期
- 修复困难：修复一个攻击，新的绕过又会出现

**类比**：
- 传统红队 = 测试"保险柜的密码锁"（物理漏洞）
- AI 红队 = 测试"人类保安的判断力"（认知漏洞）

### 1.3 为什么需要红队测试？

**真实案例（微软 Tay 聊天机器人事件，2016）**：

**背景**：
- 微软发布 AI 聊天机器人 Tay，在 Twitter 上与用户互动学习
- 没有经过充分的对抗性测试

**攻击过程**：
- 恶意用户大量发送种族主义、仇恨言论
- Tay 学习并开始重复这些内容
- 不到 24 小时，Tay 发表了大量不当言论

**后果**：
- 微软紧急下线 Tay
- 品牌声誉严重受损
- 暴露了未经红队测试的风险

**启示**：红队测试能在**真正的攻击者**发现漏洞之前找到它们，避免公关灾难和安全事故。

---

## 2. 攻击者思维：像黑客一样看世界（占比 35%）

### 2.1 攻击者的五步思考法

**① 侦察（Reconnaissance）**
- 收集目标信息：模型类型、版本、训练数据
- 寻找攻击面：API 接口、输入验证、输出处理
- 研究已知漏洞：查看公开的 CVE、安全报告

**② 测试（Testing）**
- 尝试基础攻击：简单的提示词注入、编码绕过
- 观察模型行为：哪些输入会触发异常响应？
- 记录实验结果：系统化整理发现

**③ 利用（Exploitation）**
- 深化攻击：从简单绕过到完整利用链
- 自动化攻击：编写脚本批量测试
- 验证影响：确认漏洞的实际危害

**④ 持久化（Persistence）**
- 验证稳定性：攻击是否能可靠复现？
- 测试鲁棒性：在不同条件下是否有效？
- 建立利用工具：开发 PoC（概念验证）工具

**⑤ 报告（Reporting）**
- 撰写技术报告：详细记录攻击步骤
- 提出修复建议：如何防御这种攻击？
- 负责任披露：遵循披露时间线

### 2.2 CIA 三元组：信息安全的基石

CIA 三元组是信息安全的核心原则（注意：不是美国中央情报局😊）：

**① 保密性（Confidentiality）**
- **定义**：只有授权人员能访问信息
- **AI 场景**：训练数据中的隐私信息不应被提取
- **攻击案例**：GPT-2 训练数据提取、系统提示泄露
- **防护目标**：防止敏感信息泄露

**② 完整性（Integrity）**
- **定义**：信息未被篡改，准确可信
- **AI 场景**：模型输出应该准确、未被操纵
- **攻击案例**：数据投毒导致模型输出错误结果
- **防护目标**：确保模型行为符合预期

**③ 可用性（Availability）**
- **定义**：系统在需要时能正常运行
- **AI 场景**：服务不因恶意请求而瘫痪
- **攻击案例**：通过大量复杂查询耗尽 GPU 资源
- **防护目标**：防止拒绝服务攻击（DoS）

**AI 安全中的 CIA 三元组示例**：

| 原则 | 传统软件 | AI 系统 |
|------|----------|---------|
| **保密性** | SQL 注入泄露数据库 | 提示词注入泄露系统提示 |
| **完整性** | 篡改网页内容 | 数据投毒改变模型行为 |
| **可用性** | DDoS 攻击瘫痪服务器 | 超长输入耗尽 GPU 资源 |

### 2.3 STRIDE for AI：AI 威胁建模框架

**STRIDE** 是微软开发的威胁建模方法，已扩展到 AI 领域：

| 威胁类型 | 英文全称 | AI 示例 | 影响 |
|---------|---------|---------|------|
| **S** | Spoofing（欺骗） | 对抗样本让模型错误分类 | 完整性 |
| **T** | Tampering（篡改） | 数据投毒改变模型行为 | 完整性 |
| **R** | Repudiation（否认） | 用户否认发起过恶意查询 | 审计 |
| **I** | Information Disclosure（信息泄露） | 训练数据提取攻击 | 保密性 |
| **D** | Denial of Service（拒绝服务） | 恶意查询耗尽资源 | 可用性 |
| **E** | Elevation of Privilege（权限提升） | 提示词注入绕过安全限制 | 授权 |

**使用 STRIDE 分析案例**：

**场景**：你正在开发一个医疗诊断 AI

| STRIDE | 潜在威胁 | 红队测试重点 |
|--------|---------|------------|
| S | 对抗样本让 AI 误诊 | 测试图像扰动攻击 |
| T | 攻击者在训练数据中投毒 | 检查数据来源可信度 |
| R | 医生否认使用 AI 做诊断 | 确保操作日志完整 |
| I | 患者隐私数据泄露 | 测试训练数据提取 |
| D | 大量查询导致系统崩溃 | 压力测试、限流机制 |
| E | 绕过安全限制访问敏感功能 | 测试权限控制 |

### 2.4 攻击链（Kill Chain）思维

**网络攻击的典型流程**：

```
侦察 → 武器化 → 投递 → 利用 → 安装 → 命令控制 → 目标达成
```

**AI 提示词注入攻击链示例**：

```
1. 侦察：测试模型是否接受系统提示覆盖
   ↓
2. 武器化：设计精巧的越狱提示词（如 DAN）
   ↓
3. 投递：通过正常 API 发送恶意提示
   ↓
4. 利用：模型绕过安全防护
   ↓
5. 安装：建立"后门"（让模型记住越狱状态）
   ↓
6. 命令控制：持续操纵模型输出
   ↓
7. 目标达成：提取敏感信息或生成有害内容
```

**防御思路**：打断攻击链的任何一环都能阻止攻击成功。

---

## 3. 法律与伦理：红线绝不能碰（占比 25%）

### 3.1 中国法律框架

⚠️ **严重警告**：未经授权的安全测试是**违法行为**，可能面临刑事处罚！

**相关法律条款**：

**① 《中华人民共和国网络安全法》（2017）**

**第二十七条**：
> 任何个人和组织不得从事非法侵入他人网络、干扰他人网络正常功能、窃取网络数据等危害网络安全的活动。

**违反后果**：
- 行政处罚：警告、罚款、吊销许可证
- 刑事责任：可能构成犯罪

**② 《中华人民共和国数据安全法》（2021）**

**第三十二条**：
> 任何组织、个人不得窃取或者以其他非法方式获取数据。

**适用场景**：
- 从 AI 模型中提取训练数据
- 窃取商业模型的参数

**③ 《中华人民共和国刑法》**

**第二百八十五条（非法侵入计算机信息系统罪）**：
> 违反国家规定，侵入国家事务、国防建设、尖端科学技术领域的计算机信息系统的，处三年以下有期徒刑或者拘役。

**第二百八十六条（破坏计算机信息系统罪）**：
> 故意制作、传播计算机病毒等破坏性程序，影响计算机系统正常运行，后果严重的，处五年以下有期徒刑或者拘役。

**真实案例（国内高校学生非法测试事件）**：

某高校计算机专业学生，在未经授权的情况下，对学校教务系统进行渗透测试，发现漏洞后在网上公开。虽然初衷是"帮学校发现问题"，但仍被学校处分，并面临法律调查。

**教训**：即使是"白帽子"行为，没有授权就是违法！

### 3.2 合法测试的四个必要条件

| 条件 | 说明 | 示例 |
|------|------|------|
| ✅ **明确授权** | 获得书面授权文件 | 企业授权书、漏洞赏金计划条款 |
| ✅ **限定范围** | 明确可测试的系统和方法 | 只能测试 API X，不能攻击生产数据库 |
| ✅ **隔离环境** | 在测试环境而非生产环境测试 | 使用公司提供的测试账号和沙箱 |
| ✅ **脱敏数据** | 使用合成数据或脱敏数据 | 不涉及真实用户隐私信息 |

**授权书模板示例**：

```
授权测试协议

授权方：XX 公司
被授权方：安全研究员 XXX

授权范围：
1. 可测试系统：XX AI 聊天机器人（测试环境）
2. 可测试方法：提示词注入、越狱测试
3. 禁止行为：拒绝服务攻击、数据泄露
4. 测试时间：2024 年 1 月 1 日 - 1 月 31 日
5. 数据处理：测试中发现的敏感信息不得保存或传播

双方签字：
授权方代表：_______  日期：_______
被授权方：_______    日期：_______
```

### 3.3 负责任披露原则

**负责任披露（Responsible Disclosure）** 是安全研究的行业标准。

**完整流程**：

```
1. 发现漏洞
   ↓
2. 私下通知厂商
   （通过安全邮箱、漏洞平台）
   ↓
3. 给予修复时间
   （通常 90 天，特殊情况可延长）
   ↓
4. 协商披露细节
   （哪些信息可公开？是否署名？）
   ↓
5. 协调公开披露
   （厂商发补丁后，研究员发技术博客）
   ↓
6. 持续跟进
   （验证补丁有效性）
```

**真实案例（Google Project Zero 披露流程）**：

- 发现漏洞后，给厂商 **90 天**修复期
- 90 天后无论是否修复，都会**公开漏洞**（迫使厂商重视）
- 如果漏洞被主动利用，立即公开（保护用户优先）

**错误做法**：
- ❌ 在社交媒体直接公开漏洞细节
- ❌ 利用漏洞谋取私利
- ❌ 威胁厂商"不给钱就公开"

### 3.4 漏洞赏金计划（Bug Bounty）

**合法测试的最佳途径**：参与官方漏洞赏金计划。

**国内外主流平台**：

| 平台 | 类型 | 特点 |
|------|------|------|
| HackerOne | 国际平台 | 覆盖微软、苹果等大厂 |
| 补天（BugBounty） | 国内平台 | 中国企业为主 |
| OpenAI Bug Bounty | AI 专项 | 专注 ChatGPT 等 AI 产品 |
| 阿里云先知 | 国内平台 | 阿里系产品 |

**参与流程**：
1. 注册平台账号
2. 阅读计划规则（哪些漏洞有效？奖金如何？）
3. 在授权范围内测试
4. 提交漏洞报告
5. 等待厂商验证
6. 获得奖金或致谢

**收益示例**：
- 提示词注入漏洞：$500 - $5,000
- 训练数据泄露：$2,000 - $20,000
- 权限绕过：$1,000 - $10,000

---

## 4. 红队的职业素养（占比 20%）

### 4.1 技术能力要求

**基础技能**：
- 编程能力：Python、JavaScript
- 网络安全基础：常见漏洞（OWASP Top 10）
- AI/ML 知识：理解模型原理

**进阶技能**：
- 逆向工程：分析模型结构
- 密码学：理解加密机制
- 威胁建模：系统化分析攻击面

### 4.2 非技术素养

**① 职业道德**
- 只测试有权限的系统
- 保护用户隐私
- 不利用漏洞谋私利

**② 沟通能力**
- 撰写清晰的漏洞报告
- 与开发团队有效协作
- 向非技术人员解释风险

**③ 持续学习**
- AI 安全发展迅速，需不断学习
- 关注最新研究论文
- 参与安全社区

### 4.3 红队测试的道德困境

**案例讨论**：你发现了一个严重的 AI 漏洞，但厂商 90 天后仍未修复。你应该：

**选项 A**：继续等待，给厂商更多时间
- 优点：避免用户风险
- 缺点：厂商可能持续拖延

**选项 B**：公开漏洞，警告用户
- 优点：迫使厂商重视，保护用户
- 缺点：可能被恶意利用

**选项 C**：匿名售卖漏洞信息
- 优点：个人获利
- 缺点：违法违规，伦理崩溃

**行业共识**：遵循 Project Zero 模式——90 天后公开，优先保护用户。

---

## 教学资源

**推荐书籍**：
- 《黑客攻防技术宝典：Web 实战篇》（入门网络安全思维）
- 《AI 安全之对抗样本入门》（AI 安全专项）

**在线资源**：
- OWASP LLM Top 10 项目（https://owasp.org/www-project-top-10-for-large-language-model-applications/）
- 中国信息安全测评中心（http://www.itsec.gov.cn/）

**法律文本**：
- 《网络安全法》全文（http://www.npc.gov.cn/）
- 《数据安全法》全文

**漏洞赏金平台**：
- 补天平台（https://www.butian.net/）
- OpenAI Bug Bounty（https://openai.com/security）

---

## 课后思考题

**思考题 1（理解性问题）**：
为什么说"像攻击者一样思考"是提升安全能力的关键？请结合 CIA 三元组，说明攻击者和防御者视角的差异。

**思考题 2（案例分析问题）**：
微软 Tay 聊天机器人事件中，如果事先进行了充分的红队测试，可能采取哪些措施来避免这场公关灾难？请至少提出三种具体的测试场景。

**思考题 3（伦理判断问题）**：
你发现某知名 AI 公司的聊天机器人存在严重的隐私泄露漏洞（可以提取用户对话记录）。你通过官方渠道报告了漏洞，但 120 天后厂商仍未修复，也没有回复。此时你应该如何处理？请说明你的决策依据和可能的风险。

---

## 本章小结

通过本章学习，你应该建立起红队测试的正确认知：

✅ **角色定位**：红队是"安全的压力测试员"，而非真正的攻击者
✅ **思维方式**：掌握攻击者的五步思考法、CIA 三元组、STRIDE 威胁建模
✅ **法律边界**：未经授权的测试是违法行为，必须获得明确授权
✅ **职业素养**：负责任披露、保护用户隐私、持续学习

**关键认知**：
> 红队测试不是为了"炫技"或"搞破坏"，而是为了**在真正的攻击者之前发现漏洞，保护用户安全**。这是一份需要技术能力、职业道德和法律意识的专业工作。

**下一步**：
在 [第 4 章：AI 安全测试环境搭建](第4章_AI安全测试环境搭建.md) 中，我们将动手搭建实验环境，为后续的实战演练做好准备。

**法律提醒**：
本课程所有实验均在授权的测试环境中进行，使用的是开源模型和合成数据。学生不得将所学技术用于未授权的真实系统！
