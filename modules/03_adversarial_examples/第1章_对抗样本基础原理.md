# 第 1 章：对抗样本基础原理

**章节目标**
- 建立对对抗样本的直观认识
- 理解对抗样本为什么能够欺骗 AI 模型
- 认识对抗样本在现实世界中的威胁
- 了解对抗样本与传统攻击的区别

---

## 1. 什么是对抗样本（25%）

### 1.1 从一个惊人的实验说起

2013年，研究人员发现了一个令人震惊的现象：

在一张熊猫图片上添加一层**人眼几乎看不见的噪声**，就能让世界顶级的图像识别 AI 把熊猫识别成长臂猿，而且置信度高达 99.3%！

```
[熊猫图片]  +  [微小噪声]  =  [看起来还是熊猫]
   ↓              ↓              ↓
AI: 熊猫      人眼看不见      AI: 长臂猿 99.3%
```

**这就是对抗样本（Adversarial Example）**——经过精心设计的输入，对人类来说看起来正常，但能让 AI 模型产生严重错误。

### 1.2 对抗样本的定义

对抗样本是指：通过对正常输入添加**微小但精心计算的扰动**，使机器学习模型产生错误输出的样本。

**关键特征**：
- **扰动微小**：人眼难以察觉变化
- **效果显著**：能完全改变模型的判断
- **精心设计**：不是随机噪声，而是有针对性的修改

### 1.3 生活化类比

**类比一：视觉魔术**

对抗样本就像魔术师的障眼法。魔术师通过微小的手法变化，让观众"看到"完全不同的东西。AI 模型就像被骗的观众——它"看到"的和真实情况完全不同。

**类比二：人类的视觉错觉**

想想那些经典的视觉错觉图——两条一样长的线看起来不一样长。对抗样本就是专门针对 AI 设计的"错觉图"，只不过这种错觉对人类无效，只对 AI 有效。

### 1.4 对抗样本 vs 传统攻击

| 对比维度 | 传统软件攻击 | 对抗样本攻击 |
|---------|-------------|-------------|
| **攻击对象** | 代码逻辑漏洞 | 模型决策边界 |
| **攻击方式** | 构造恶意输入触发 bug | 构造视觉相似但语义不同的输入 |
| **修复难度** | 修补代码即可 | 需要重新训练模型 |
| **检测难度** | 可通过代码审计发现 | 难以预测所有可能的对抗样本 |

---

## 2. 为什么对抗样本能成功（30%）

### 2.1 深度学习的"阿喀琉斯之踵"

要理解对抗样本，需要先理解深度学习模型是如何"看"图片的。

**人类的视觉理解**：
- 识别物体的**整体形状、轮廓、结构**
- 关注**语义特征**（熊猫有黑眼圈、圆耳朵）
- 对微小变化**鲁棒**（熊猫换个姿势还是熊猫）

**AI 模型的"视觉理解"**：
- 关注**像素级的统计模式**
- 可能依赖人类不注意的**纹理特征**
- 对特定方向的微小变化**高度敏感**

### 2.2 决策边界的脆弱性

想象模型的决策过程是在一个高维空间中划分区域：

```
        熊猫区域          长臂猿区域
    ┌──────────┐      ┌──────────┐
    │          │      │          │
    │   🐼 ←───┼──────┼→  🦧     │
    │  原图    │ 边界 │  对抗样本 │
    │          │      │          │
    └──────────┘      └──────────┘
```

**关键洞察**：
- 两个类别之间的**决策边界**可能非常接近正常样本
- 只需要在特定方向上移动很小的距离，就能"跨越"边界
- 这个"特定方向"就是对抗扰动的方向

### 2.3 高维空间的诅咒

在高维空间（如图像的像素空间）中：
- 每个像素都是一个维度
- 一张 224×224 的彩色图片有 **150,528 个维度**
- 在如此高维的空间中，到边界的"捷径"几乎总是存在的

**直觉理解**：维度越高，能找到"逃逸路径"的方向就越多。

### 2.4 为什么扰动可以很小

研究发现，对抗扰动通常只需要改变每个像素 **1-5 个灰度级**（256 级中的）。

**原因**：
- 模型对某些方向的变化**极度敏感**
- 这些敏感方向与模型的**梯度方向**密切相关
- 梯度告诉我们"往哪个方向走，模型的输出变化最大"

---

## 3. 对抗样本的分类（20%）

### 3.1 按攻击目标分类

**无目标攻击（Untargeted Attack）**：
- 只要模型判断错误就算成功
- 熊猫 → 任何非熊猫的类别
- 相对容易实现

**有目标攻击（Targeted Attack）**：
- 要求模型输出特定的错误类别
- 熊猫 → 必须识别为长臂猿
- 更有实际危害（如人脸识别绕过）

### 3.2 按攻击者知识分类

**白盒攻击（White-box Attack）**：
- 攻击者知道模型的全部信息（结构、参数、梯度）
- 可以直接计算最优扰动
- 攻击效果最好

**黑盒攻击（Black-box Attack）**：
- 攻击者只能查询模型的输入输出
- 不知道模型内部结构
- 更符合现实场景

### 3.3 按扰动范围分类

| 类型 | 描述 | 示例 |
|-----|------|------|
| **L∞ 扰动** | 每个像素最大改变量 | 每个像素最多变化 8 |
| **L2 扰动** | 总体扰动能量限制 | 整体欧氏距离小于阈值 |
| **L0 扰动** | 修改的像素数量限制 | 只修改 100 个像素 |

### 3.4 数字世界 vs 物理世界

**数字对抗样本**：
- 直接修改图像文件
- 扰动可以精确控制到每个像素
- 用于攻击在线服务

**物理对抗样本**：
- 在现实世界中实现（如打印的贴纸、特殊涂装）
- 需要应对光照、角度、距离变化
- 更难实现，但威胁更大

---

## 4. 真实世界的对抗样本威胁（25%）

### 4.1 案例一：自动驾驶停车标志攻击（2018年）

**事件背景**：
研究人员在停车标志上贴上特制的"涂鸦贴纸"，使自动驾驶系统的图像识别模块将其错误识别为"限速45"标志。

**攻击实现**：
- 贴纸图案经过优化，在多种角度和距离下都有效
- 对人类来说，标志看起来只是有些"涂鸦"
- 但 AI 视觉系统会产生致命误判

**潜在危害**：
- 车辆在该停车的路口不停车
- 可能导致严重交通事故

**防御启示**：
- 自动驾驶不能仅依赖单一感知模块
- 需要多传感器融合验证

### 4.2 案例二：人脸识别对抗眼镜（2016年）

**事件背景**：
卡内基梅隆大学研究人员设计了特殊图案的眼镜框，戴上后可以：
- 让人脸识别系统**无法识别**佩戴者
- 或者将佩戴者**误识别为其他人**

**攻击实现**：
```
[正常人脸] + [对抗眼镜] = [被识别为另一个人]
```

**实际威胁**：
- 绕过门禁系统的人脸验证
- 逃避监控摄像头的追踪
- 冒充他人通过身份验证

### 4.3 案例三：3D 打印对抗物体（2019年）

研究人员创建了 3D 打印的对抗物体：
- 一个看起来像海龟的物体，被 AI 识别为步枪
- 一个看起来像棒球的物体，被识别为咖啡杯

**意义**：对抗样本不仅限于 2D 图片，在 3D 物理世界同样有效。

### 4.4 威胁总结

| 应用场景 | 潜在攻击 | 危害程度 |
|---------|---------|---------|
| **自动驾驶** | 误判交通标志、障碍物 | 🔴 致命 |
| **人脸识别** | 绕过身份验证、冒充他人 | 🔴 高 |
| **医疗诊断** | 误判病灶、漏诊 | 🔴 高 |
| **内容审核** | 绕过违规内容检测 | 🟡 中 |
| **垃圾邮件** | 绕过垃圾邮件过滤 | 🟢 低 |

---

## 本章小结

### 核心要点回顾

1. **对抗样本的定义**：添加微小扰动使模型严重误判的输入
2. **成功原因**：模型决策边界脆弱 + 高维空间的敏感方向
3. **分类方式**：按目标（有/无目标）、按知识（白盒/黑盒）、按扰动范围
4. **现实威胁**：自动驾驶、人脸识别、医疗诊断等关键领域

### 与实验的衔接

在 **实验 3.1：FGSM 白盒攻击** 中，你将：
- 亲手生成你的第一个对抗样本
- 观察微小扰动如何改变模型判断
- 理解梯度方向与对抗扰动的关系

---

## 课后思考题

1. **理解性问题**：为什么对抗样本对人类视觉无效，但对 AI 模型有效？这说明 AI "看"世界的方式和人类有什么本质区别？

2. **分析性问题**：物理世界的对抗样本（如对抗贴纸）比数字对抗样本更难实现，具体有哪些额外挑战？

3. **设计性问题**：如果你是自动驾驶系统的安全工程师，面对对抗样本威胁，你会设计什么样的多层防御策略？

---

## 扩展阅读

- **开创性论文**：Goodfellow et al.《Explaining and Harnessing Adversarial Examples》(2015)
- **物理对抗样本**：Eykholt et al.《Robust Physical-World Attacks on Deep Learning Models》(2018)
- **科普视频**：搜索 "Adversarial Examples Explained" 了解可视化演示

---

**法律与伦理提醒**：
对抗样本研究应用于提升 AI 系统的鲁棒性和安全性。将对抗样本技术用于攻击真实系统（如自动驾驶、人脸识别门禁）可能构成犯罪。请始终在授权的研究环境中进行实验。
