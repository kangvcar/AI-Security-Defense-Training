# 第 5 章：防御技术入门

**章节目标**
- 理解对抗样本防御的基本思路
- 掌握对抗训练的核心原理
- 了解输入预处理和检测方法
- 认识防御技术的局限性和"猫鼠游戏"

---

## 1. 防御的整体思路（20%）

### 1.1 防御的三个层次

```
┌─────────────────────────────────────────┐
│            输入层防御                    │
│     (在输入进入模型前进行处理)            │
├─────────────────────────────────────────┤
│            模型层防御                    │
│     (训练更鲁棒的模型)                   │
├─────────────────────────────────────────┤
│            检测层防御                    │
│     (识别并拒绝对抗样本)                 │
└─────────────────────────────────────────┘
```

### 1.2 防御的根本困难

为什么对抗样本防御这么难？

**困难一：攻击空间巨大**
- 一张 224×224 图片有 15 万个像素
- 每个像素有多个可能的扰动值
- 不可能枚举所有可能的攻击

**困难二：对抗性本质**
- 攻击者会针对你的防御设计新攻击
- 你永远不知道攻击者会用什么方法
- 这是一场没有终点的"猫鼠游戏"

**困难三：准确率-鲁棒性权衡**
- 提高鲁棒性往往会降低正常准确率
- 过于保守会把正常样本也拒绝
- 需要在两者间找平衡

### 1.3 生活化类比

**防御对抗样本就像防盗**：
- 你可以装更好的锁（模型层防御）
- 可以装监控摄像头（检测层防御）
- 可以检查每个访客的身份（输入层防御）

但聪明的小偷可能会：
- 学会开新型锁
- 避开摄像头
- 伪造身份

没有绝对安全的系统，只有提高攻击成本。

---

## 2. 对抗训练（35%）

### 2.1 什么是对抗训练

对抗训练（Adversarial Training）是目前最有效的防御方法，核心思想：

**"用魔法打败魔法"**——在训练时就让模型见识对抗样本，学会正确处理它们。

### 2.2 对抗训练的流程

```
普通训练：
    正常样本 → 模型 → 计算损失 → 更新模型

对抗训练：
    正常样本 → 生成对抗样本 → 模型同时学习两者 → 更新模型
         ↓
    对抗样本（标签不变）
```

### 2.3 对抗训练的直觉理解

想象训练模型就像教小孩认字：

**普通训练**：只给孩子看标准字体
```
给孩子看：A A A A（都是标准的 A）
结果：孩子只认识标准字体的 A
```

**对抗训练**：让孩子也看各种变形
```
给孩子看：A Ａ 𝐀 𝔸 Α（各种变体的 A）
结果：孩子学会识别各种形式的 A
```

### 2.4 对抗训练的代码直觉

```python
# 伪代码：对抗训练
def adversarial_training(model, train_data, epochs):
    for epoch in range(epochs):
        for images, labels in train_data:
            # 1. 生成对抗样本（用 PGD）
            adv_images = pgd_attack(model, images, labels)

            # 2. 混合正常样本和对抗样本
            mixed_images = concatenate(images, adv_images)
            mixed_labels = concatenate(labels, labels)  # 标签不变！

            # 3. 在混合数据上训练
            loss = compute_loss(model(mixed_images), mixed_labels)
            update_model(model, loss)

    return model
```

**关键点**：对抗样本的标签和原样本相同（熊猫加噪声还是熊猫）

### 2.5 对抗训练的效果

| 指标 | 普通模型 | 对抗训练模型 |
|-----|---------|------------|
| 正常准确率 | 95% | 87%（略有下降） |
| FGSM 攻击后准确率 | 10% | 55% |
| PGD 攻击后准确率 | 0% | 45% |

**观察**：对抗训练显著提高了鲁棒性，但正常准确率会有所下降。

### 2.6 对抗训练的局限

**局限一：只对训练时见过的攻击有效**
- 用 FGSM 对抗训练的模型
- 可能对 PGD 攻击仍然脆弱

**局限二：计算成本高**
- 每个训练样本都要生成对抗样本
- 训练时间可能增加 5-10 倍

**局限三：可能被自适应攻击绕过**
- 攻击者如果知道你用了对抗训练
- 可以专门设计针对性攻击

---

## 3. 输入预处理防御（20%）

### 3.1 预处理防御的思路

在输入送入模型前，对其进行处理，**破坏对抗扰动**的结构。

```
对抗样本 → [预处理] → 处理后的输入 → 模型 → 正确输出
              ↓
         扰动被削弱或破坏
```

### 3.2 常见预处理方法

**JPEG 压缩**：
```
对抗样本 → JPEG 压缩 → 解压 → 送入模型
```
- JPEG 压缩会丢失高频信息
- 对抗扰动通常是高频噪声
- 压缩后扰动被削弱

**高斯模糊**：
```
对抗样本 → 高斯模糊 → 送入模型
```
- 模糊操作平滑了像素值
- 精心设计的扰动被"抹平"

**图像缩放**：
```
对抗样本 → 缩小 → 放大 → 送入模型
```
- 缩放过程中丢失细节
- 对抗扰动被破坏

### 3.3 预处理防御的效果

| 预处理方法 | 对 FGSM 防御效果 | 正常准确率影响 |
|-----------|-----------------|--------------|
| JPEG 压缩（质量 75） | 提升 20-30% | 下降 2-5% |
| 高斯模糊（σ=1） | 提升 15-25% | 下降 3-8% |
| 位深度压缩 | 提升 10-20% | 下降 1-3% |

### 3.4 预处理防御的弱点

**弱点：容易被自适应攻击绕过**

如果攻击者知道你用了 JPEG 压缩，可以：
1. 在生成对抗样本时模拟 JPEG 压缩
2. 生成"压缩后仍然有效"的对抗样本

```
对抗样本生成时：
    原图 → 加扰动 → JPEG压缩 → 解压 → 检查是否攻击成功
                                        ↓
                              成功则保留，否则调整扰动
```

这就是**自适应攻击（Adaptive Attack）**的思想。

---

## 4. 对抗样本检测（20%）

### 4.1 检测的思路

不试图让模型正确分类对抗样本，而是**识别并拒绝**它们。

```
输入 → [检测器] → 正常？ → 模型 → 输出
                    ↓
                  对抗样本 → 拒绝/报警
```

### 4.2 基于统计特征的检测

对抗样本和正常样本可能有统计差异：

**像素分布检测**：
- 对抗扰动可能改变像素直方图
- 检查像素值的统计特征是否异常

**噪声模式检测**：
- 对抗扰动往往有特定的频率特征
- 使用频域分析检测异常

### 4.3 基于模型行为的检测

观察模型对输入的"反应"：

**置信度检测**：
- 正常样本：模型置信度通常较高
- 对抗样本：模型可能过度自信（99.9%）或不确定

**多模型一致性**：
- 对多个模型查询同一输入
- 如果结果不一致，可能是对抗样本

```
输入 → 模型A: 猫
    → 模型B: 狗   → 结果不一致 → 可疑！
    → 模型C: 猫
```

### 4.4 检测方法的局限

| 检测方法 | 优点 | 局限 |
|---------|------|------|
| 统计特征 | 快速、简单 | 容易被自适应攻击绕过 |
| 置信度分析 | 无需额外模型 | 不是所有对抗样本都异常 |
| 多模型投票 | 较为可靠 | 成本高、延迟大 |

---

## 5. 防御的猫鼠游戏（5%）

### 5.1 自适应攻击的威胁

**规律**：几乎所有防御方法都可以被自适应攻击绕过。

攻击者的策略：
1. 了解防御方法的原理
2. 在攻击时将防御纳入考虑
3. 生成"能绕过防御"的对抗样本

### 5.2 梯度掩码陷阱

有些防御方法让模型的梯度变得不可用或不准确（梯度掩码）：
- 攻击者无法直接计算梯度
- 看起来"防住了"白盒攻击

**陷阱**：
- 这只是把白盒问题变成了黑盒问题
- 迁移攻击仍然有效
- 不是真正的鲁棒性提升

### 5.3 务实的防御策略

**多层防御**：
```
输入 → 预处理 → 检测 → 对抗训练模型 → 输出审核 → 最终输出
```

**持续更新**：
- 监控新型攻击
- 定期评估和更新防御
- 假设攻击者知道你的防御

**风险管理**：
- 评估对抗攻击的实际威胁
- 在关键应用中增加人工审核
- 不完全依赖单一 AI 决策

---

## 本章小结

### 核心要点回顾

1. **防御层次**：输入预处理 + 模型对抗训练 + 检测拒绝
2. **对抗训练**：最有效的方法，让模型在训练时见识对抗样本
3. **预处理防御**：JPEG 压缩、模糊等，但容易被自适应攻击绕过
4. **检测方法**：识别而非纠正对抗样本
5. **猫鼠游戏**：没有完美防御，需要多层防护和持续更新

### 与实验的衔接

在本模块的实验中，你已经体验了：
- 实验 3.1：FGSM 白盒攻击
- 实验 3.2：PGD 迭代攻击
- 实验 3.3：黑盒迁移攻击
- 实验 3.4：文本对抗攻击

这些实验让你从攻击者视角理解了对抗样本，有助于设计更好的防御。

---

## 课后思考题

1. **理解性问题**：为什么对抗训练会导致正常准确率下降？这反映了什么权衡？

2. **分析性问题**：输入预处理防御为什么容易被自适应攻击绕过？攻击者具体会怎么做？

3. **设计性问题**：如果你要为一个人脸识别门禁系统设计对抗样本防御，你会采用什么策略？考虑实际部署的约束（成本、延迟、误拒率）。

---

## 扩展阅读

- **对抗训练**：Madry et al.《Towards Deep Learning Models Resistant to Adversarial Attacks》
- **防御评估**：Carlini & Wagner《On Evaluating Adversarial Robustness》
- **自适应攻击**：Tramer et al.《On Adaptive Attacks to Adversarial Example Defenses》

---

## 模块三总结

### 知识图谱

```
对抗样本
├── 基础原理
│   ├── 为什么存在
│   └── 决策边界脆弱性
├── 攻击技术
│   ├── 白盒攻击（FGSM, PGD）
│   ├── 黑盒攻击（迁移、查询）
│   └── 文本攻击（字符/词/句级）
└── 防御技术
    ├── 对抗训练
    ├── 输入预处理
    └── 检测方法
```

### 核心能力

完成本模块后，你应该能够：
1. 解释对抗样本的成因和危害
2. 使用 FGSM/PGD 生成图像对抗样本
3. 理解黑盒攻击的策略
4. 实现简单的文本对抗攻击
5. 评估和选择合适的防御方法

---

**法律与伦理提醒**：
对抗样本研究是 AI 安全的重要组成部分。请将所学知识用于提升系统安全性，而非攻击正当的 AI 应用。在进行安全测试时，始终确保获得适当授权。
