# 第 3 章：黑盒攻击技术

**章节目标**
- 理解黑盒攻击的现实意义和挑战
- 掌握迁移攻击的原理和实现方法
- 了解基于查询的攻击策略
- 认识黑盒攻击在实际场景中的应用

---

## 1. 黑盒攻击概述（25%）

### 1.1 什么是黑盒攻击

黑盒攻击（Black-box Attack）是指攻击者**不知道目标模型的内部信息**，只能通过输入输出来进行攻击。

**攻击者的限制**：
- ❌ 不知道模型结构
- ❌ 不知道模型参数
- ❌ 无法计算梯度
- ✅ 可以向模型发送查询
- ✅ 可以获得模型的输出（预测结果或置信度）

### 1.2 为什么黑盒攻击更重要

**现实场景**：大多数商业 AI 服务都是"黑盒"：

| 服务类型 | 示例 | 攻击者能获得的信息 |
|---------|------|------------------|
| **云端 API** | Google Vision、百度 AI | 输入图片 → 返回标签和置信度 |
| **移动应用** | 人脸解锁、拍照识物 | 输入图片 → 返回识别结果 |
| **嵌入式系统** | 自动驾驶摄像头 | 只能观察系统行为 |

**关键问题**：在不知道模型细节的情况下，如何生成有效的对抗样本？

### 1.3 黑盒攻击的两大策略

**策略一：迁移攻击（Transfer Attack）**
- 在本地训练一个"替代模型"
- 用白盒方法攻击替代模型
- 希望对抗样本对目标模型也有效

**策略二：查询攻击（Query-based Attack）**
- 反复查询目标模型
- 根据返回结果逐步优化扰动
- 不需要替代模型，但需要大量查询

### 1.4 生活化类比

**迁移攻击**：就像你要参加某个老师的考试，但不知道他会出什么题。于是你找了另一个类似的老师，把他的题目都做了一遍，希望两个老师的出题风格相似。

**查询攻击**：就像玩"猜数字"游戏——你猜一个数，对方说"大了"或"小了"，你根据反馈不断调整，最终猜中。

---

## 2. 迁移攻击（35%）

### 2.1 迁移性的发现

研究人员发现了一个惊人的现象：**对抗样本具有迁移性**。

```
在模型 A 上生成的对抗样本
        ↓
对模型 B、C、D 也可能有效！
```

**为什么会有迁移性**：
- 不同模型可能学到了相似的特征
- 决策边界的脆弱方向可能相似
- 深度学习模型有共同的"盲点"

### 2.2 迁移攻击的流程

```
步骤 1：收集与目标任务相似的数据
        ↓
步骤 2：训练一个本地"替代模型"
        ↓
步骤 3：用白盒方法（FGSM/PGD）攻击替代模型
        ↓
步骤 4：将生成的对抗样本送给目标模型
        ↓
步骤 5：观察攻击是否成功
```

### 2.3 提高迁移性的技巧

**技巧一：使用多个替代模型**

```
替代模型 1 ─┐
替代模型 2 ─┼→ 综合生成对抗样本 → 目标模型
替代模型 3 ─┘
```

对多个模型都有效的扰动，更可能对目标模型也有效。

**技巧二：增加输入多样性**

在生成对抗样本时，对输入进行随机变换（缩放、平移、旋转），让扰动更加"通用"。

**技巧三：攻击中间层特征**

不只攻击最终输出，还攻击模型的中间层表示，这些表示更可能在不同模型间相似。

### 2.4 迁移攻击的成功率

| 替代模型 → 目标模型 | 迁移成功率 |
|-------------------|-----------|
| 相同架构（如 ResNet → ResNet） | 60-80% |
| 相似架构（如 ResNet → VGG） | 40-60% |
| 不同架构（如 CNN → ViT） | 20-40% |
| 多模型集成攻击 | 50-70% |

### 2.5 案例：攻击商业人脸识别 API

**场景**：攻击者想绕过某商业人脸识别服务

**步骤**：
1. 下载公开的人脸识别模型（如 FaceNet）
2. 收集目标人物的照片
3. 用 PGD 生成对抗样本，让 FaceNet 认不出
4. 将对抗样本上传到商业 API 测试

**结果**：研究表明，约 40-60% 的对抗样本能成功迁移到商业 API。

---

## 3. 基于查询的攻击（25%）

### 3.1 查询攻击的思想

如果能反复查询目标模型，就可以通过**试错**来找到有效的扰动。

**核心问题**：如何用最少的查询次数找到有效攻击？

### 3.2 基于评分的攻击

如果模型返回**置信度分数**，可以用优化方法：

```
初始化：随机扰动
        ↓
循环：
    ├─ 向目标模型查询当前对抗样本
    ├─ 获得置信度分数
    ├─ 根据分数调整扰动方向
    └─ 重复直到攻击成功或达到查询限制
```

**常用方法**：
- **随机搜索**：尝试随机方向，保留效果好的
- **进化算法**：模拟自然选择，优胜劣汰
- **梯度估计**：用有限差分近似梯度

### 3.3 基于决策的攻击

如果模型只返回**最终标签**（没有置信度），攻击更困难：

**HopSkipJump 算法思想**：

1. 从一个已知的对抗样本出发（如随机噪声图片）
2. 逐步向原图靠近，同时保持"攻击成功"状态
3. 找到决策边界上距离原图最近的点

```
原图 ●────────────────● 随机对抗样本
      ↑              ↑
    正常区域    攻击成功区域

目标：找到边界上最近的点
      ●───────●─────────●
      原图   边界点   随机样本
```

### 3.4 查询效率对比

| 攻击方法 | 平均查询次数 | 成功率 | 适用场景 |
|---------|------------|-------|---------|
| **随机搜索** | 10,000+ | 低 | 简单模型 |
| **NES（进化策略）** | 1,000-5,000 | 中 | 有置信度返回 |
| **HopSkipJump** | 5,000-20,000 | 高 | 只有标签返回 |

### 3.5 查询攻击的现实限制

**限制一：API 调用成本**
- 商业 API 按查询次数收费
- 10,000 次查询可能花费数百元

**限制二：查询速率限制**
- 服务商可能限制每分钟查询次数
- 异常查询模式可能触发封禁

**限制三：检测机制**
- 服务商可能监测对抗攻击尝试
- 大量相似图片查询会触发警报

---

## 4. 黑盒攻击的防御与检测（15%）

### 4.1 从防御角度看黑盒攻击

**迁移攻击的防御**：
- 使用与公开模型差异大的架构
- 添加防御性蒸馏或对抗训练
- 输入预处理（压缩、降噪）

**查询攻击的防御**：
- 限制 API 查询频率
- 检测异常查询模式
- 返回模糊化的置信度（不给精确分数）

### 4.2 检测对抗攻击尝试

```python
# 简化的异常检测逻辑
def detect_attack_attempt(query_history, current_query):
    # 检测 1：短时间内大量相似查询
    if too_many_similar_queries(query_history):
        return True

    # 检测 2：查询图片有明显噪声模式
    if has_adversarial_noise_pattern(current_query):
        return True

    # 检测 3：同一用户频繁查询同一目标
    if repeated_target_queries(query_history):
        return True

    return False
```

### 4.3 攻防博弈

| 防御措施 | 攻击者的应对 |
|---------|------------|
| 限制查询频率 | 分布式查询、慢速攻击 |
| 模糊置信度 | 使用决策边界攻击 |
| 检测异常模式 | 混入正常查询、增加随机性 |
| 使用特殊架构 | 尝试更多替代模型 |

---

## 本章小结

### 核心要点回顾

1. **黑盒攻击的现实意义**：大多数商业 AI 都是黑盒，黑盒攻击更具实际威胁
2. **迁移攻击**：利用对抗样本的迁移性，在替代模型上生成，对目标模型测试
3. **查询攻击**：通过反复查询，用优化方法逐步找到有效扰动
4. **效率权衡**：迁移攻击零查询但成功率低，查询攻击成功率高但成本高

### 与实验的衔接

在 **实验 3.3：黑盒迁移攻击** 中，你将：
- 训练简单的替代模型
- 生成对抗样本并测试迁移性
- 观察不同模型间的迁移成功率

---

## 课后思考题

1. **理解性问题**：为什么对抗样本会具有迁移性？这说明不同的深度学习模型有什么共同点？

2. **分析性问题**：商业 API 返回模糊化的置信度（如只返回 "高/中/低"）如何增加攻击难度？攻击者还有什么应对方法？

3. **设计性问题**：如果你是云端 AI 服务的安全工程师，你会设计什么机制来检测和防御基于查询的对抗攻击？

---

## 扩展阅读

- **迁移攻击研究**：《Delving into Transferable Adversarial Examples and Black-box Attacks》
- **查询攻击**：《HopSkipJumpAttack: A Query-Efficient Decision-Based Attack》
- **真实 API 攻击**：《Practical Black-Box Attacks on Deep Neural Networks》

---

**法律与伦理提醒**：
对商业 AI API 进行未经授权的对抗攻击测试可能违反服务条款和法律。研究者应使用专门的测试环境或获得服务商授权。本章内容仅用于安全教育和防御研究。
