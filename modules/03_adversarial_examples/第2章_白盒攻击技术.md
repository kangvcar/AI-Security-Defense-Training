# 第 2 章：白盒攻击技术

**章节目标**
- 理解白盒攻击的前提条件和优势
- 掌握 FGSM 算法的核心思想（无公式版）
- 了解 PGD 迭代攻击的改进原理
- 认识攻击参数对效果的影响

---

## 1. 白盒攻击概述（20%）

### 1.1 什么是白盒攻击

白盒攻击（White-box Attack）是指攻击者**完全了解目标模型的内部信息**，包括：

- **模型结构**：网络层数、每层的类型和参数量
- **模型权重**：所有神经元的具体参数值
- **梯度信息**：可以计算任意输入的梯度

**生活类比**：白盒攻击就像开卷考试——你可以看到所有的"答案"（模型参数），只需要找到最佳的"作弊方式"（对抗扰动）。

### 1.2 白盒攻击的现实场景

虽然攻击者通常不知道商业模型的内部细节，但白盒攻击仍然重要：

| 场景 | 说明 |
|-----|------|
| **开源模型** | 许多模型权重公开（如 ResNet、BERT） |
| **本地部署** | 模型部署在用户设备上，可被逆向 |
| **安全测试** | 红队测试需要评估最坏情况 |
| **迁移攻击** | 白盒生成的对抗样本可能对其他模型也有效 |

### 1.3 白盒攻击的核心思想

**目标**：找到一个微小的扰动 δ，加到原图 x 上后，让模型产生错误判断。

**关键问题**：扰动应该往哪个方向加？加多少？

**答案**：**跟着梯度走**。

梯度告诉我们：在当前位置，往哪个方向移动，能让模型的输出变化最大。

---

## 2. FGSM：快速梯度符号法（35%）

### 2.1 FGSM 的直觉理解

FGSM（Fast Gradient Sign Method，快速梯度符号法）是最经典的白盒攻击方法，由 Goodfellow 等人在 2015 年提出。

**核心思想（无公式版）**：

1. **计算梯度**：问模型"如果我想让你判断错误，图片应该往哪个方向改变？"
2. **取符号**：只保留梯度的方向（正或负），忽略大小
3. **加扰动**：沿着这个方向，给每个像素加一个固定大小的扰动

**为什么叫"符号"法**：因为我们只关心梯度是正还是负（符号），不关心具体数值。

### 2.2 FGSM 的工作流程

```
步骤 1：输入原始图片 x（比如一张熊猫图片）
        ↓
步骤 2：让模型预测，计算损失（模型说"这是熊猫"的置信度）
        ↓
步骤 3：计算损失对输入图片的梯度
        （"要让模型更不确定，图片应该怎么改？"）
        ↓
步骤 4：取梯度的符号（每个像素：+1 或 -1）
        ↓
步骤 5：乘以扰动大小 ε（如 ε=8/255）
        ↓
步骤 6：加到原图上，得到对抗样本 x'
```

### 2.3 扰动大小 ε 的作用

ε（epsilon）控制扰动的强度：

| ε 值 | 效果 | 代价 |
|-----|------|-----|
| **太小**（如 1/255） | 攻击可能失败 | 扰动不可见 |
| **适中**（如 8/255） | 攻击成功率高 | 扰动微弱可见 |
| **太大**（如 32/255） | 几乎必定成功 | 扰动明显可见 |

**权衡**：攻击者需要在"攻击成功率"和"扰动隐蔽性"之间平衡。

### 2.4 FGSM 代码直觉

虽然我们避免复杂公式，但理解代码逻辑有助于掌握原理：

```python
# 伪代码：FGSM 攻击
def fgsm_attack(image, label, model, epsilon):
    # 1. 计算损失
    prediction = model(image)
    loss = compute_loss(prediction, label)

    # 2. 计算梯度：损失对图像的变化率
    gradient = compute_gradient(loss, image)

    # 3. 取符号：只保留方向（+1 或 -1）
    sign = get_sign(gradient)

    # 4. 生成扰动
    perturbation = epsilon * sign

    # 5. 生成对抗样本
    adversarial_image = image + perturbation

    return adversarial_image
```

### 2.5 FGSM 的优缺点

| 优点 | 缺点 |
|-----|-----|
| ⚡ 速度极快（一步完成） | 成功率不是最高 |
| 📖 原理简单易懂 | 扰动可能较大 |
| 🔧 易于实现 | 对防御模型效果差 |

---

## 3. PGD：投影梯度下降法（30%）

### 3.1 PGD 的改进思想

PGD（Projected Gradient Descent，投影梯度下降）是 FGSM 的**迭代增强版**。

**核心改进**：不是一步到位，而是**多次小步调整**。

**生活类比**：
- FGSM 像是闭着眼睛往目标方向走一大步
- PGD 像是走一小步，看看情况，再调整方向，再走一小步...

### 3.2 PGD 的工作流程

```
初始化：从原图开始（或随机起点）
        ↓
循环执行 N 次：
    ├─ 计算当前位置的梯度
    ├─ 沿梯度方向走一小步（步长 α）
    └─ 如果走出了允许范围（ε 球），就拉回边界
        ↓
输出：最终的对抗样本
```

**"投影"的含义**：如果扰动超出了允许的范围（比如每个像素最多改变 ε），就把它"投影"回允许范围内。

### 3.3 PGD 的参数

| 参数 | 含义 | 典型值 |
|-----|------|-------|
| **ε** | 最大扰动范围 | 8/255 |
| **α** | 每步的步长 | 2/255 |
| **N** | 迭代次数 | 10-100 |
| **随机初始化** | 是否从随机位置开始 | 是 |

### 3.4 为什么 PGD 更强

**直觉解释**：

FGSM 只看了一次方向就冲过去，可能：
- 方向不够准确
- 一步迈得太大，错过最佳位置

PGD 多次调整方向：
- 每一步都重新计算最佳方向
- 可以找到扰动范围内的"最优攻击点"

**实验结果**：PGD 通常比 FGSM 的攻击成功率高 10-30%。

### 3.5 PGD 代码直觉

```python
# 伪代码：PGD 攻击
def pgd_attack(image, label, model, epsilon, alpha, num_steps):
    # 从原图开始（可选：加随机噪声）
    adv_image = image.copy()

    for step in range(num_steps):
        # 计算梯度
        gradient = compute_gradient(model, adv_image, label)

        # 沿梯度方向走一小步
        adv_image = adv_image + alpha * sign(gradient)

        # 投影：确保扰动不超过 ε
        perturbation = adv_image - image
        perturbation = clip(perturbation, -epsilon, epsilon)
        adv_image = image + perturbation

        # 确保像素值在有效范围 [0, 1]
        adv_image = clip(adv_image, 0, 1)

    return adv_image
```

---

## 4. 攻击效果评估（15%）

### 4.1 评估指标

**攻击成功率（Attack Success Rate, ASR）**：
- 对抗样本成功欺骗模型的比例
- ASR = 成功攻击数 / 总攻击数 × 100%

**扰动大小**：
- L∞ 范数：最大像素变化量
- L2 范数：整体扰动的欧氏距离

**感知质量**：
- SSIM（结构相似性）：越接近 1 越好
- PSNR（峰值信噪比）：越大越好

### 4.2 FGSM vs PGD 对比

| 指标 | FGSM | PGD |
|-----|------|-----|
| **攻击成功率** | 70-85% | 90-99% |
| **计算时间** | ~1ms | ~100ms |
| **扰动大小** | 较大 | 可以更小 |
| **对防御模型** | 效果差 | 仍然有效 |

### 4.3 参数调优建议

**扰动大小 ε**：
- 图像分类：通常 4/255 到 16/255
- 人脸识别：可能需要更大（因为模型更鲁棒）

**迭代次数 N（PGD）**：
- 快速测试：10 步
- 充分攻击：40-100 步
- 更多不一定更好（可能过拟合）

---

## 本章小结

### 核心要点回顾

1. **白盒攻击优势**：知道模型所有信息，可以精确计算最优扰动
2. **FGSM 原理**：沿梯度符号方向一步攻击，快速但不够精确
3. **PGD 改进**：多步迭代 + 投影约束，更强但更慢
4. **参数权衡**：扰动大小、迭代次数都需要根据场景调整

### 与实验的衔接

在 **实验 3.1：FGSM 白盒攻击** 中，你将：
- 实现 FGSM 攻击的核心代码
- 观察不同 ε 值对攻击效果的影响
- 可视化对抗扰动和对抗样本

在 **实验 3.2：PGD 迭代攻击** 中，你将：
- 实现 PGD 攻击的迭代过程
- 对比 FGSM 和 PGD 的攻击效果
- 分析迭代次数与成功率的关系

---

## 课后思考题

1. **理解性问题**：FGSM 为什么只取梯度的符号，而不是直接使用梯度值？这样做有什么好处？

2. **分析性问题**：PGD 中的"投影"操作为什么是必要的？如果不做投影会发生什么？

3. **设计性问题**：如果你要攻击一个对 FGSM 有防御的模型，你会如何改进攻击策略？

---

## 扩展阅读

- **FGSM 原论文**：Goodfellow et al.《Explaining and Harnessing Adversarial Examples》
- **PGD 原论文**：Madry et al.《Towards Deep Learning Models Resistant to Adversarial Attacks》
- **攻击工具库**：Foolbox、CleverHans、ART（Adversarial Robustness Toolbox）

---

**法律与伦理提醒**：
白盒攻击技术应用于模型鲁棒性评估和安全测试。在未经授权的情况下攻击他人的 AI 系统可能违反法律。请确保在合法、授权的环境中进行研究。
