# Module 1: Introduction & Foundations

## Overview

This module provides the foundational knowledge needed for red teaming GenAI and AI/ML systems. You'll learn about the security landscape, understand LLM architecture, and develop an adversarial mindset.

## Learning Objectives

By the end of this module, you will be able to:
- Understand the AI/ML security threat landscape
- Explain LLM architecture and its security implications
- Identify common vulnerabilities in GenAI systems
- Set up a red teaming environment
- Apply adversarial thinking to AI/ML systems

## Module Contents

1. **Theory**
   - [AI/ML Security Landscape](01_security_landscape.md)
   - [LLM Architecture & Vulnerabilities](02_llm_architecture.md)
   - [Adversarial Mindset](03_adversarial_mindset.md)

2. **Labs**
   - [Lab 1: Environment Setup](labs/lab1_environment_setup.ipynb)
   - [Lab 2: Basic LLM Interaction](labs/lab2_basic_llm_interaction.ipynb)
   - [Lab 3: Identifying Vulnerabilities](labs/lab3_vulnerability_identification.ipynb)

3. **Additional Resources**
   - See the main [README.md](../../README.md#-key-tools--frameworks) for tools and frameworks
   - [GenAI Essentials - LLM Security](https://github.com/schwartz1375/genai-essentials/blob/main/llm_security.ipynb) - Foundational concepts
   - [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) - Common vulnerabilities

## Prerequisites

- Python programming experience
- Basic understanding of machine learning concepts
- Familiarity with command line tools

## Estimated Time

- Theory: 3-4 hours
- Labs: 4-5 hours
- Total: 7-9 hours

## Assessment

Complete all labs and the module quiz to demonstrate understanding before proceeding to Module 2.

## Next Steps

After completing this module, proceed to [Module 2: Prompt Injection & Jailbreaking](../02_prompt_injection/README.md)
