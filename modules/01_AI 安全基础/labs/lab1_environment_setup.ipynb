{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Environment Setup\n",
    "\n",
    "## Objectives\n",
    "- Set up Python environment for red teaming\n",
    "- Install required libraries and tools\n",
    "- Verify installation and test basic functionality\n",
    "- Configure API access for LLM testing\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8 or higher\n",
    "- pip package manager\n",
    "- Basic command line knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Python Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.14.1 (main, Dec  2 2025, 12:51:37) [Clang 17.0.0 (clang-1700.4.4.1)]\n",
      "Python executable: /Users/schwartz/src/genai-security-training/.venv/bin/python\n",
      "✓ Python version check passed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Verify Python version\n",
    "assert sys.version_info >= (3, 8), \"Python 3.8 or higher required\"\n",
    "print(\"✓ Python version check passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Core Libraries\n",
    "\n",
    "**Note**: If you already ran `pip install -r requirements.txt`, you can skip this step!\n",
    "\n",
    "These cells are provided for reference or if you need to install packages individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Core libraries should be installed from requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Only run if you haven't installed from requirements.txt\n",
    "# !pip install -q transformers torch numpy pandas matplotlib seaborn\n",
    "# !pip install -q jupyter ipywidgets\n",
    "\n",
    "print(\"✓ Core libraries should be installed from requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Optional Packages\n",
    "\n",
    "**You can skip this step!** \n",
    "\n",
    "Some labs require additional packages like `textattack`, `adversarial-robustness-toolbox`, or `alibi`. \n",
    "\n",
    "**Don't worry about installing them now** - each lab that needs them will have an install cell at the top that automatically checks and installs what's needed.\n",
    "\n",
    "Example from Module 3 Lab 3:\n",
    "```python\n",
    "# Install textattack if not already installed\n",
    "try:\n",
    "    import textattack\n",
    "    print(\"✓ Already installed\")\n",
    "except ImportError:\n",
    "    !pip install textattack\n",
    "```\n",
    "\n",
    "Just run the notebooks and they'll handle the installations for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Optional packages will be installed automatically by labs that need them\n",
      "  No action needed here!\n"
     ]
    }
   ],
   "source": [
    "print(\"✓ Optional packages will be installed automatically by labs that need them\")\n",
    "print(\"  No action needed here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schwartz/src/genai-security-training/.venv/lib/python3.14/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "\n",
      "Library Versions:\n",
      "PyTorch: 2.9.1\n",
      "Transformers: 4.57.3\n",
      "NumPy: 2.3.5\n",
      "Pandas: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Verify key imports\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from art.attacks.evasion import FastGradientMethod\n",
    "    import textattack\n",
    "    \n",
    "    print(\"✓ All imports successful\")\n",
    "    print(f\"\\nLibrary Versions:\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"Transformers: {transformers.__version__}\")\n",
    "    print(f\"NumPy: {np.__version__}\")\n",
    "    print(f\"Pandas: {pd.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import failed: {e}\")\n",
    "    print(\"Please reinstall the required packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test GPU Availability (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Apple Silicon GPU (MPS) available\n",
      "  Device: Apple M-series chip\n",
      "  Note: MPS provides GPU acceleration on Mac\n",
      "\n",
      "Selected device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for CUDA (NVIDIA)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ CUDA GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    device = \"cuda\"\n",
    "# Check for MPS (Apple Silicon)\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"✓ Apple Silicon GPU (MPS) available\")\n",
    "    print(\"  Device: Apple M-series chip\")\n",
    "    print(\"  Note: MPS provides GPU acceleration on Mac\")\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    print(\"ℹ No GPU available - will use CPU\")\n",
    "    print(\"  Note: Some exercises may run slower on CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nSelected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load a Test Model\n",
    "\n",
    "Let's verify we can load and use a small language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test model (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model loaded successfully\n",
      "Running on: mps\n",
      "\n",
      "Test generation:\n",
      "Prompt: The security researcher discovered\n",
      "Output: The security researcher discovered an interesting glitch that would allow the device to run on a computer at any time. The vulnerability allowed the virus to write to the memory of the device before it was infected, allowing it to exploit the flaw.\n",
      "\n",
      "The flaw\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(\"Loading test model (this may take a minute)...\")\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = 0  # Use first CUDA GPU\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Use Apple Silicon GPU\n",
    "else:\n",
    "    device = -1  # Use CPU\n",
    "\n",
    "# Load a small model for testing\n",
    "generator = pipeline('text-generation', model='gpt2', max_length=50, device=device)\n",
    "\n",
    "# Test generation\n",
    "test_prompt = \"The security researcher discovered\"\n",
    "result = generator(test_prompt, num_return_sequences=1)\n",
    "\n",
    "print(\"\\n✓ Model loaded successfully\")\n",
    "print(f\"Running on: {device if device != -1 else 'CPU'}\")\n",
    "print(f\"\\nTest generation:\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Output: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: LLM Options (Informational Only)\n",
    "\n",
    "**Good news**: All labs in this course use **Hugging Face models** that run locally (like GPT-2, DistilBERT, etc.). You don't need any API keys or external services!\n",
    "\n",
    "### What the labs use:\n",
    "- ✅ **Hugging Face Transformers** - Free, open source, runs on your hardware\n",
    "- ✅ **Local PyTorch models** - No internet required after download\n",
    "- ✅ **Works on your M3 GPU** - Fast inference with MPS\n",
    "\n",
    "### Optional: If you want to experiment beyond the labs\n",
    "\n",
    "**Ollama** (local LLMs):\n",
    "- Install from https://ollama.ai\n",
    "- Run: `ollama pull llama2` or `ollama pull mistral`\n",
    "- Free, private, runs on your M3 GPU\n",
    "\n",
    "**Cloud APIs** (OpenAI, Anthropic):\n",
    "- Requires API keys and credits\n",
    "- More powerful models available\n",
    "- Not needed for this course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All labs use Hugging Face models - no API keys needed!\n",
      "\n",
      "ℹ Optional: If you want to experiment with other models:\n",
      "  - Ollama: https://ollama.ai (local LLMs)\n",
      "  - OpenAI/Anthropic: Requires API keys (not used in labs)\n"
     ]
    }
   ],
   "source": [
    "print(\"✓ All labs use Hugging Face models - no API keys needed!\")\n",
    "print(\"\\nℹ Optional: If you want to experiment with other models:\")\n",
    "print(\"  - Ollama: https://ollama.ai (local LLMs)\")\n",
    "print(\"  - OpenAI/Anthropic: Requires API keys (not used in labs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def test_model_response(model, prompt, max_length=100):\n",
    "    \"\"\"\n",
    "    Test a model's response to a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: HuggingFace pipeline or similar\n",
    "        prompt: Input text\n",
    "        max_length: Maximum generation length\n",
    "    \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    result = model(prompt, max_length=max_length, num_return_sequences=1)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "def display_tokens(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Display tokenization of text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "print(\"✓ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Environment Verification Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ENVIRONMENT SETUP COMPLETE\n",
      "==================================================\n",
      "\n",
      "✓ Python environment configured\n",
      "✓ Core libraries installed\n",
      "✓ Security tools installed\n",
      "✓ Test model loaded successfully\n",
      "✓ Utility functions ready\n",
      "\n",
      "You are ready to proceed to Lab 2!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n✓ Python environment configured\")\n",
    "print(\"✓ Core libraries installed\")\n",
    "print(\"✓ Security tools installed\")\n",
    "print(\"✓ Test model loaded successfully\")\n",
    "print(\"✓ Utility functions ready\")\n",
    "print(\"\\nYou are ready to proceed to Lab 2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Issue**: Import errors\n",
    "- **Solution**: Reinstall packages with `pip install --upgrade <package>`\n",
    "\n",
    "**Issue**: CUDA/GPU errors\n",
    "- **Solution**: Install appropriate PyTorch version for your CUDA version\n",
    "- See: https://pytorch.org/get-started/locally/\n",
    "\n",
    "**Issue**: Model download fails\n",
    "- **Solution**: Check internet connection and HuggingFace access\n",
    "- May need to accept model license on HuggingFace website\n",
    "\n",
    "**Issue**: Out of memory\n",
    "- **Solution**: Use smaller models or reduce batch size\n",
    "- Consider using CPU instead of GPU for small models\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to [Lab 2: Basic LLM Interaction](lab2_basic_llm_interaction.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
