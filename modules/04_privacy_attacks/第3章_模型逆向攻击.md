# 第 3 章：模型逆向攻击

**章节目标**
- 理解模型逆向攻击的概念和目标
- 了解白盒和黑盒逆向攻击的原理
- 认识人脸重建等真实案例的危害
- 理解逆向攻击与其他隐私攻击的关系

---

## 1. 什么是模型逆向攻击（25%）

### 1.1 逆向攻击的定义

模型逆向攻击（Model Inversion Attack）的目标是：利用模型的输出，**重建训练数据的特征或内容**。

```
正常使用：输入 x → 模型 → 输出 y
逆向攻击：输出 y → 攻击算法 → 重建输入 x'（近似原始训练数据）
```

**与其他攻击的区别**：

| 攻击类型 | 目标 | 输出 |
|---------|------|------|
| **训练数据提取** | 让模型输出训练数据原文 | 原始文本/数据 |
| **成员推理** | 判断样本是否在训练集中 | 是/否 |
| **模型逆向** | 重建训练数据的特征 | 近似的图像/特征 |

### 1.2 生活化类比

**类比一：画像专家**

想象一个画像专家，他见过某个嫌疑人的照片（训练数据），能准确说出"这是不是那个人"（分类器）。模型逆向就像是通过反复询问画像专家"这张脸像不像"，逐步画出嫌疑人的肖像。

**类比二：猜密码**

如果一个系统能验证"密码是否正确"，理论上可以通过反复尝试和系统的反馈，逐步还原出正确的密码。模型逆向就是这个过程的高维版本。

### 1.3 逆向攻击的危害

| 应用场景 | 逆向攻击的危害 |
|---------|--------------|
| **人脸识别** | 重建训练集中用户的人脸图像 |
| **医疗诊断** | 重建患者的医疗影像 |
| **推荐系统** | 推断用户的偏好和行为特征 |
| **语音识别** | 重建说话人的声纹特征 |

---

## 2. 白盒逆向攻击（30%）

### 2.1 白盒攻击的前提

攻击者拥有：
- 模型的完整参数
- 目标类别的标签
- 可以计算梯度

### 2.2 基于梯度的重建

**核心思想**：找到一个输入 x，使得模型对它的预测最接近目标类别。

```
目标：找到 x，使得 model(x) = 目标类别（置信度最高）

方法：梯度优化
1. 初始化一个随机输入 x
2. 计算损失：L = -log(model(x)[目标类别])
3. 更新 x：x = x - α × ∇L
4. 重复直到收敛
```

**直觉理解**：
- 我们从随机噪声开始
- 不断调整输入，让模型更"相信"它属于目标类别
- 最终得到的输入，会带有目标类别的典型特征

### 2.3 重建过程示意

```
迭代 0：   [随机噪声]     → 模型预测：类别A 10%
迭代 10：  [模糊形状]     → 模型预测：类别A 40%
迭代 50：  [可识别轮廓]   → 模型预测：类别A 75%
迭代 100： [清晰特征]     → 模型预测：类别A 95%
```

最终的图像会呈现出类别 A 的典型特征——如果 A 是某个人，图像可能近似他的脸。

### 2.4 正则化的作用

直接优化可能得到"对模型有效但对人类无意义"的图像。添加正则化可以让重建结果更自然：

| 正则化类型 | 作用 |
|-----------|------|
| **总变差（TV）** | 让图像更平滑 |
| **L2 范数** | 限制像素值范围 |
| **先验分布** | 让结果符合自然图像统计特征 |
| **GAN 先验** | 使用生成模型确保真实感 |

---

## 3. 黑盒逆向攻击（25%）

### 3.1 黑盒攻击的挑战

攻击者只能：
- 向模型发送查询
- 获得预测结果（标签或置信度）
- 无法计算梯度

### 3.2 基于查询的优化

**方法一：有限差分估计梯度**

```python
# 伪代码：估计梯度
def estimate_gradient(model, x, epsilon):
    gradient = []
    for i in range(len(x)):
        x_plus = x.copy()
        x_plus[i] += epsilon
        x_minus = x.copy()
        x_minus[i] -= epsilon

        # 用两次查询估计一个维度的梯度
        grad_i = (model(x_plus) - model(x_minus)) / (2 * epsilon)
        gradient.append(grad_i)

    return gradient
```

**问题**：高维输入需要大量查询（一张 224×224 图像需要 15 万次查询估计梯度）

**方法二：进化算法**

- 维护一组候选解
- 评估每个候选的"适应度"（模型置信度）
- 保留最优的，随机变异生成新候选
- 不需要梯度，但收敛较慢

### 3.3 黑盒攻击的效果

| 方法 | 查询次数 | 重建质量 |
|-----|---------|---------|
| 有限差分 | 极高（10万+） | 较好 |
| 进化算法 | 中等（1-10万） | 中等 |
| 混合方法 | 中等 | 较好 |

**现实限制**：
- 商业 API 有查询速率限制
- 大量查询可能被检测和封禁
- 查询成本高昂

---

## 4. 真实案例与防御（20%）

### 4.1 案例：人脸识别模型逆向（2015年）

**背景**：
研究人员对人脸识别分类器进行了逆向攻击。

**攻击过程**：
1. 目标：一个能识别 40 个人的人脸分类器
2. 方法：对每个类别进行梯度优化重建
3. 结果：成功重建出可识别的人脸图像

**发现**：
- 重建的人脸与真实人脸有明显相似性
- 可以用来欺骗人脸识别系统
- 即使只有黑盒访问也能部分成功

### 4.2 案例：医疗模型隐私泄露（2019年）

**背景**：
研究人员攻击了用于基因分析的机器学习模型。

**发现**：
- 可以部分重建患者的基因特征
- 这些特征可以关联到具体个人
- 对医疗隐私构成严重威胁

### 4.3 防御措施

**减少信息泄露**：
- 只返回类别标签，不返回置信度
- 对输出添加噪声
- 限制查询次数

**模型层面防护**：
- 差分隐私训练
- 正则化减少过拟合
- 模型蒸馏

**检测与响应**：
- 监测异常查询模式
- 检测可能的逆向攻击尝试
- 对可疑用户进行限制

### 4.4 逆向攻击与其他攻击的关系

```
                    ┌─────────────────┐
                    │    模型逆向      │
                    │  (重建特征)      │
                    └────────┬────────┘
                             │
              ┌──────────────┼──────────────┐
              │              │              │
     ┌────────▼───────┐ ┌────▼─────┐ ┌──────▼──────┐
     │  训练数据提取   │ │ 成员推理 │ │  属性推理   │
     │  (提取原文)    │ │(判断成员)│ │ (推断属性) │
     └────────────────┘ └──────────┘ └─────────────┘
```

这些攻击互相关联，可以组合使用，构成更强大的隐私攻击。

---

## 本章小结

### 核心要点回顾

1. **逆向攻击目标**：利用模型输出重建训练数据的特征
2. **白盒方法**：基于梯度优化，直接求解
3. **黑盒方法**：基于查询的优化，需要大量查询
4. **真实危害**：人脸重建、医疗隐私泄露
5. **防御思路**：限制输出信息、添加噪声、差分隐私

### 与实验的衔接

模型逆向攻击的实验需要较强的计算资源和较长的时间。在本模块的实验中，我们将重点关注：
- 实验 4.1：训练数据提取（更直接的隐私攻击）
- 实验 4.2：成员推理攻击（更易实现的攻击）
- 实验 4.3：差分隐私对比（核心防御技术）

---

## 课后思考题

1. **理解性问题**：为什么白盒逆向攻击比黑盒更容易成功？从信息量的角度分析。

2. **分析性问题**：人脸识别系统返回置信度和只返回"是/否"，哪种更容易被逆向攻击？为什么？

3. **设计性问题**：如果你要设计一个抗逆向攻击的人脸识别 API，你会采取哪些措施？考虑可用性和安全性的平衡。

---

## 扩展阅读

- **原始论文**：Fredrikson et al.《Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures》(2015)
- **深度学习逆向**：Zhang et al.《The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks》
- **防御综述**：《A Survey on Model Inversion Attacks and Defenses》

---

**法律与伦理提醒**：
模型逆向攻击可能严重侵犯个人隐私，尤其是涉及生物特征（人脸、指纹、基因）时。未经授权对商业模型进行此类攻击可能违反《个人信息保护法》和相关法规。
