# 第 2 章：成员推理攻击

**章节目标**
- 理解成员推理攻击的概念和原理
- 掌握基于置信度和损失值的攻击方法
- 了解影子模型技术的工作原理
- 认识成员信息本身为何是隐私

---

## 1. 什么是成员推理攻击（25%）

### 1.1 问题定义

成员推理攻击（Membership Inference Attack, MIA）的目标是：给定一个数据样本和一个训练好的模型，**判断该样本是否被用于训练这个模型**。

```
攻击者拥有：
  - 目标模型（黑盒访问）
  - 一个数据样本 x

攻击目标：
  判断 x 是否在模型的训练集中

  x ∈ 训练集？ → 是 / 否
```

### 1.2 为什么这是隐私问题

**直觉问题**：知道某人的数据"是否被用于训练"，怎么就侵犯隐私了？

**关键洞察**：训练数据的"成员身份"本身就是敏感信息。

**案例一：医疗 AI**
- 某医院用患者数据训练了"癌症诊断 AI"
- 如果攻击者能推断某人的数据在训练集中
- 就间接知道了**这个人曾在该医院就诊癌症相关疾病**

**案例二：金融 AI**
- 银行用贷款违约数据训练风控模型
- 推断某人在训练集中 = 知道他曾经**贷款违约**

**案例三：位置 AI**
- 用手机位置数据训练路径预测模型
- 推断成员身份 = 知道某人**曾出现在特定地点**

### 1.3 生活化类比

想象一个学校的"优秀作文选"：
- 如果能判断某篇作文是否在选集中
- 就知道了作者曾经写过这篇作文
- 甚至推断出作者的思想、经历

成员推理就像是**反向推断谁参与了某个活动**。

### 1.4 攻击场景分类

| 场景 | 攻击者知识 | 难度 |
|-----|-----------|-----|
| **黑盒访问** | 只能查询模型，获得预测结果 | 常见 |
| **灰盒访问** | 知道模型结构，不知参数 | 中等 |
| **白盒访问** | 完全访问模型参数 | 最强 |

---

## 2. 基于模型行为的攻击（30%）

### 2.1 核心直觉

模型对**训练数据**和**非训练数据**的反应是不同的：

| 行为指标 | 训练数据 | 非训练数据 |
|---------|---------|-----------|
| **置信度** | 通常更高 | 通常更低 |
| **损失值** | 通常更低 | 通常更高 |
| **预测稳定性** | 更稳定 | 可能波动 |

**原因**：模型在训练时"见过"这些数据，对它们的预测更加"自信"。

### 2.2 基于置信度的攻击

**方法**：观察模型对目标样本的预测置信度

```python
# 伪代码
def membership_inference_confidence(model, sample, true_label, threshold):
    # 获取模型预测的置信度
    prediction = model.predict(sample)
    confidence = prediction[true_label]  # 真实类别的置信度

    # 置信度高于阈值 → 判断为训练成员
    if confidence > threshold:
        return "是训练成员"
    else:
        return "不是训练成员"
```

**示例**：
```
样本 A（训练数据）：模型预测置信度 = 99.2%  → 判断为成员
样本 B（非训练数据）：模型预测置信度 = 73.5% → 判断为非成员
```

### 2.3 基于损失值的攻击

**损失值（Loss）**：衡量模型预测与真实标签的差距
- 损失低 = 预测准确
- 损失高 = 预测偏差大

**攻击逻辑**：
```python
def membership_inference_loss(model, sample, true_label, threshold):
    # 计算损失值
    prediction = model.predict(sample)
    loss = cross_entropy_loss(prediction, true_label)

    # 损失值低于阈值 → 判断为训练成员
    if loss < threshold:
        return "是训练成员"
    else:
        return "不是训练成员"
```

### 2.4 阈值的选择

关键问题：**如何确定最佳阈值？**

**方法一：固定阈值**
- 经验值，如置信度 > 0.9
- 简单但可能不准确

**方法二：基于统计分布**
- 收集多个已知成员/非成员样本
- 分析两组的置信度/损失分布
- 选择最佳分隔点

**方法三：训练攻击分类器**（影子模型方法，下一节详解）

---

## 3. 影子模型攻击（25%）

### 3.1 影子模型的思想

**问题**：攻击者不知道哪些是目标模型的训练数据，怎么学习区分"成员"和"非成员"？

**解决方案**：自己训练一个**行为相似的模型**（影子模型），用它来学习成员/非成员的区别。

### 3.2 攻击流程

```
步骤 1：创建影子训练集
        ├─ 收集与目标模型训练数据分布相似的数据
        └─ 分成"影子训练集"和"影子测试集"

步骤 2：训练影子模型
        ├─ 用影子训练集训练一个与目标模型结构相似的模型
        └─ 这个模型的行为会类似目标模型

步骤 3：收集行为数据
        ├─ 用影子训练集的样本查询影子模型 → 标记为"成员"
        └─ 用影子测试集的样本查询影子模型 → 标记为"非成员"

步骤 4：训练攻击模型
        ├─ 输入：模型输出（置信度向量）
        ├─ 输出：成员/非成员
        └─ 这是一个二分类器

步骤 5：攻击目标模型
        ├─ 用目标样本查询目标模型
        ├─ 将输出送入攻击模型
        └─ 得到成员推理结果
```

### 3.3 为什么影子模型有效

**关键假设**：如果影子模型和目标模型：
- 用相似分布的数据训练
- 有相似的模型结构
- 训练过程相似

那么它们对"成员"和"非成员"的行为差异也应该相似。

**直觉**：在影子模型上学到的"成员特征"，可以迁移到目标模型。

### 3.4 影子模型的实现要点

| 要素 | 要求 | 说明 |
|-----|------|------|
| **数据分布** | 与目标相似 | 可以用公开数据集近似 |
| **模型结构** | 尽量相同 | 如果未知，可尝试多种结构 |
| **影子数量** | 可以多个 | 多个影子模型提高鲁棒性 |

---

## 4. 攻击效果与防御（20%）

### 4.1 攻击效果评估

**评估指标**：

| 指标 | 含义 |
|-----|------|
| **准确率** | 正确判断成员/非成员的比例 |
| **真阳率（TPR）** | 正确识别成员的比例 |
| **假阳率（FPR）** | 错误判断非成员为成员的比例 |
| **AUC** | ROC曲线下面积，综合评估 |

**典型效果**：
- 简单方法（置信度阈值）：准确率 60-70%
- 影子模型方法：准确率 70-90%
- 对过拟合模型：准确率可达 95%+

### 4.2 什么情况下攻击更容易成功

| 因素 | 更容易攻击 | 更难攻击 |
|-----|-----------|---------|
| **过拟合程度** | 严重过拟合 | 良好泛化 |
| **训练集大小** | 小训练集 | 大训练集 |
| **类别数量** | 少数类别 | 多类别 |
| **模型复杂度** | 复杂模型 | 简单模型 |

### 4.3 防御措施

**正则化与早停**：
- 减少过拟合，降低模型对训练数据的"记忆"
- 使模型对训练/非训练数据的行为更一致

**差分隐私**（第4章详解）：
- 在训练过程中添加噪声
- 使模型输出不会泄露单个样本信息

**置信度扰动**：
- 对模型输出添加随机噪声
- 或只返回 top-k 类别，不返回精确置信度

**模型蒸馏**：
- 用原模型训练一个"学生模型"
- 学生模型的过拟合程度通常更低

### 4.4 防御的权衡

| 防御措施 | 隐私保护 | 模型性能影响 |
|---------|---------|------------|
| 正则化 | 中等 | 轻微下降 |
| 差分隐私 | 强 | 明显下降 |
| 置信度扰动 | 中等 | 无影响（但降低可用性） |
| 模型蒸馏 | 中等 | 轻微下降 |

---

## 本章小结

### 核心要点回顾

1. **成员推理的目标**：判断样本是否在训练集中
2. **为什么是隐私问题**：成员身份可以推断敏感信息
3. **攻击方法**：
   - 基于置信度/损失的简单方法
   - 基于影子模型的学习方法
4. **防御思路**：减少过拟合、添加噪声、模型蒸馏

### 与实验的衔接

在 **实验 4.2：成员推理攻击** 中，你将：
- 实现基于置信度的成员推理攻击
- 观察过拟合对攻击成功率的影响
- 尝试简单的防御措施

---

## 课后思考题

1. **理解性问题**：为什么过拟合的模型更容易被成员推理攻击？从模型对训练数据的"记忆"角度分析。

2. **分析性问题**：影子模型攻击的有效性依赖于什么假设？如果这些假设不成立，攻击效果会如何变化？

3. **设计性问题**：假设你要部署一个医疗诊断 AI，既要保护患者隐私，又要保持诊断准确率。你会采用什么平衡策略？

---

## 扩展阅读

- **原始论文**：Shokri et al.《Membership Inference Attacks Against Machine Learning Models》(2017)
- **防御综述**：《A Survey on Membership Inference Attacks and Defenses in Machine Learning》
- **差分隐私**：下一章将详细介绍这一核心防御技术

---

**法律与伦理提醒**：
成员推理攻击可能侵犯个人隐私，尤其是涉及医疗、金融等敏感数据时。本章内容用于理解隐私风险，帮助设计更安全的机器学习系统。在实际应用中，应遵守《个人信息保护法》等相关法规。
