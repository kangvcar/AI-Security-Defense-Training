# 第 4 章：差分隐私基础

**章节目标**
- 理解差分隐私的核心思想和直觉
- 掌握隐私预算（ε）的含义
- 了解 DP-SGD 训练的基本原理
- 认识差分隐私在工业界的实际应用

---

## 1. 为什么需要差分隐私（25%）

### 1.1 传统隐私保护的困境

**常见的"伪隐私保护"方法**：

| 方法 | 看似保护了隐私 | 实际问题 |
|-----|--------------|---------|
| **删除姓名** | 看不到名字了 | 可通过其他特征重新识别 |
| **数据脱敏** | 敏感字段被掩盖 | 模式仍可被学习 |
| **聚合统计** | 只发布平均值 | 差分攻击可推断个体 |
| **访问控制** | 限制谁能看数据 | 不能阻止模型泄露 |

**案例：Netflix 数据集去匿名化**

2006年，Netflix 发布了"匿名化"的用户观影数据用于推荐算法比赛。研究人员通过与 IMDB 公开评分交叉对比，成功**重新识别**了具体用户，还原了他们的完整观影记录。

**核心问题**：传统方法保护的是数据本身，而不是数据携带的**信息**。

### 1.2 差分隐私的核心思想

差分隐私（Differential Privacy, DP）的目标是：

**无论你的数据在不在数据集中，输出结果几乎没有区别。**

```
数据集 A = {用户1, 用户2, 用户3, ...}
数据集 B = {用户1, 用户2, ...}        （少了用户3）

差分隐私保证：
  分析(A) ≈ 分析(B)

→ 用户3 的加入或退出不会显著影响输出
→ 攻击者无法从输出推断用户3 的信息
```

### 1.3 生活化类比

**类比：投票隐私**

想象一个公司要调查"员工是否满意薪资"。如果直接统计，可能从变化中推断某人的回答。

**差分隐私的做法**：
1. 每个人在回答前抛一枚硬币
2. 正面：如实回答
3. 反面：再抛一次，正面回答"满意"，反面回答"不满意"

这样，任何单个回答都有随机性，无法确定某人的真实想法，但整体统计仍然有意义。

### 1.4 差分隐私的数学定义（直觉版）

一个算法 M 满足 ε-差分隐私，如果：

**对于任意两个相邻数据集（只差一条记录），算法的输出分布几乎相同。**

"几乎相同"由 ε 控制：
- ε 越小 → 分布越接近 → 隐私保护越强
- ε 越大 → 分布差异越大 → 隐私保护越弱

---

## 2. 隐私预算（ε）的理解（25%）

### 2.1 ε 的直觉含义

ε（epsilon）被称为"隐私预算"或"隐私损失"：

| ε 值 | 隐私保护程度 | 直觉理解 |
|-----|------------|---------|
| **0.1** | 极强 | 几乎无法区分你在不在数据集中 |
| **1.0** | 强 | 攻击者获得的信息很有限 |
| **10** | 较弱 | 攻击者可能推断出部分信息 |
| **∞** | 无保护 | 完全暴露 |

### 2.2 ε 与攻击成功率的关系

如果一个查询满足 ε-差分隐私，攻击者的成功率被限制为：

- 攻击者猜测正确的概率最多提升 e^ε 倍
- 当 ε = 1 时，概率最多提升约 2.7 倍
- 当 ε = 0.1 时，概率最多提升约 1.1 倍

### 2.3 隐私预算的"消耗"

**关键性质**：每次查询都会消耗隐私预算。

```
第1次查询：消耗 ε₁
第2次查询：消耗 ε₂
第3次查询：消耗 ε₃
...
总隐私损失 ≤ ε₁ + ε₂ + ε₃ + ...
```

**这意味着**：
- 查询次数越多，隐私泄露越多
- 需要预先规划总隐私预算
- 预算用完后不能再查询

### 2.4 隐私-效用权衡

```
        隐私保护程度
             ↑
             │     ε = 0.1
             │     ★ （高隐私，低效用）
             │
             │         ε = 1.0
             │         ★ （平衡点）
             │
             │              ε = 10
             │              ★ （低隐私，高效用）
             └──────────────────────→ 数据/模型效用
```

**核心权衡**：更强的隐私保护 = 更多的噪声 = 更低的数据效用

---

## 3. DP-SGD：差分隐私训练（30%）

### 3.1 问题：如何训练满足差分隐私的模型

普通的机器学习训练过程：
```
数据 → 计算梯度 → 更新模型 → 训练好的模型
```

问题：模型可能"记住"训练数据，导致隐私泄露。

### 3.2 DP-SGD 的核心思想

DP-SGD（Differentially Private Stochastic Gradient Descent）通过两个操作保护隐私：

**操作1：梯度裁剪（Gradient Clipping）**

限制每个样本对梯度的贡献，防止单个异常值主导更新。

```python
# 伪代码
for each sample in batch:
    gradient = compute_gradient(sample)
    # 如果梯度太大，按比例缩小
    if norm(gradient) > C:
        gradient = gradient * (C / norm(gradient))
```

**操作2：添加噪声（Noise Addition）**

在聚合梯度后添加高斯噪声，隐藏单个样本的影响。

```python
# 伪代码
batch_gradient = sum(clipped_gradients) / batch_size
# 添加校准过的噪声
noisy_gradient = batch_gradient + Gaussian(0, σ²)
# 用加噪后的梯度更新模型
model = model - learning_rate * noisy_gradient
```

### 3.3 DP-SGD 工作流程

```
┌─────────────────────────────────────────────────────────┐
│                    每个训练步骤                          │
├─────────────────────────────────────────────────────────┤
│ 1. 随机采样一个 batch                                    │
│ 2. 对每个样本计算梯度                                    │
│ 3. 裁剪每个样本的梯度（限制范围）                         │
│ 4. 计算 batch 的平均梯度                                 │
│ 5. 添加高斯噪声                                          │
│ 6. 用加噪梯度更新模型                                    │
│ 7. 记录累计隐私损失                                      │
└─────────────────────────────────────────────────────────┘
```

### 3.4 DP-SGD 的参数

| 参数 | 含义 | 影响 |
|-----|------|------|
| **C（裁剪阈值）** | 单个梯度的最大范数 | 太小会丢失信息，太大保护不够 |
| **σ（噪声标准差）** | 添加噪声的强度 | 越大隐私越好，但学习越难 |
| **batch_size** | 每批样本数 | 越大噪声效果越明显 |
| **epochs** | 训练轮数 | 越多隐私消耗越大 |

### 3.5 隐私预算的计算

DP-SGD 的总隐私损失取决于：
- 噪声水平 σ
- 采样率（batch_size / 总样本数）
- 训练步数

可以使用专门的工具（如 Opacus、TensorFlow Privacy）自动计算。

---

## 4. 工业应用与实践（20%）

### 4.1 Google 的 RAPPOR

**应用场景**：Chrome 浏览器收集用户统计信息

**方法**：
- 本地差分隐私（在用户设备上加噪）
- 用户数据在离开设备前就被保护
- Google 只能看到加噪后的数据

**效果**：
- 保护了单个用户的隐私
- 仍能进行大规模统计分析

### 4.2 Apple 的差分隐私

**应用场景**：
- 表情符号使用统计
- 健康数据收集
- Safari 浏览统计

**特点**：
- 强调本地差分隐私
- 在设备端完成隐私保护
- 服务器从未见过原始数据

### 4.3 差分隐私的实际效果

**实验对比**：

| 配置 | 模型准确率 | 隐私保护 |
|-----|-----------|---------|
| 无差分隐私 | 95% | 无 |
| ε = 10 | 92% | 较弱 |
| ε = 1 | 85% | 强 |
| ε = 0.1 | 70% | 极强 |

**观察**：更强的隐私保护确实会降低模型性能，但对于大数据集，影响相对较小。

### 4.4 何时使用差分隐私

**推荐使用**：
- 处理敏感个人数据（医疗、金融）
- 需要发布统计结果
- 法规要求隐私保护

**可能不需要**：
- 数据本身是公开的
- 模型不会被外部访问
- 隐私风险可接受

---

## 本章小结

### 核心要点回顾

1. **差分隐私的目标**：单个数据的加入/退出不影响输出
2. **ε 的含义**：隐私预算，越小保护越强
3. **DP-SGD**：梯度裁剪 + 噪声添加
4. **隐私-效用权衡**：更强隐私 = 更低效用
5. **工业应用**：Google、Apple 已大规模部署

### 与实验的衔接

在 **实验 4.3：差分隐私对比** 中，你将：
- 对比普通训练和 DP-SGD 训练的模型
- 观察不同 ε 值对模型准确率的影响
- 测试差分隐私对成员推理攻击的防御效果

---

## 课后思考题

1. **理解性问题**：为什么差分隐私需要添加噪声？噪声是如何保护隐私的？

2. **分析性问题**：DP-SGD 中为什么需要进行梯度裁剪？如果不裁剪会有什么问题？

3. **设计性问题**：如果你要为一个医疗 AI 系统选择隐私预算 ε，你会考虑哪些因素？如何在隐私保护和诊断准确率之间取得平衡？

---

## 扩展阅读

- **差分隐私入门**：Dwork & Roth《The Algorithmic Foundations of Differential Privacy》
- **DP-SGD 论文**：Abadi et al.《Deep Learning with Differential Privacy》
- **实践工具**：PyTorch Opacus、TensorFlow Privacy

---

## 模块四总结

### 知识图谱

```
AI 隐私攻击与防御
├── 训练数据提取
│   ├── 记忆现象
│   └── 提取方法（困惑度、补全）
├── 成员推理攻击
│   ├── 置信度/损失方法
│   └── 影子模型方法
├── 模型逆向攻击
│   ├── 白盒（梯度优化）
│   └── 黑盒（查询优化）
└── 差分隐私防御
    ├── 隐私预算 ε
    └── DP-SGD 训练
```

### 核心能力

完成本模块后，你应该能够：
1. 理解 AI 模型的隐私风险
2. 解释成员推理攻击的原理
3. 理解差分隐私的核心思想
4. 评估隐私保护的权衡

---

**法律与伦理提醒**：
差分隐私是保护用户隐私的重要技术。在处理个人敏感数据时，应遵守《个人信息保护法》等相关法规，在技术手段和法律合规两方面同时做好隐私保护。
