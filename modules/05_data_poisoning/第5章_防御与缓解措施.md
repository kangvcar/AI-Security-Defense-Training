# 第 5 章：防御与缓解措施

**章节目标**
- 掌握数据清洗和验证的基本方法
- 了解鲁棒训练技术的原理
- 认识后门移除技术（Fine-Pruning、蒸馏）
- 建立完整的供应链安全意识

---

## 1. 数据层防御（25%）

### 1.1 数据清洗

在训练前对数据进行清洗，移除可疑样本：

**异常检测方法**：

| 方法 | 原理 | 适用场景 |
|-----|------|---------|
| **统计异常** | 检测像素分布异常的样本 | 简单触发器 |
| **聚类分析** | 移除远离簇中心的样本 | 投毒样本检测 |
| **损失分析** | 移除损失值异常高/低的样本 | 标签翻转检测 |

**示例流程**：
```
原始数据集
    ↓
特征提取
    ↓
异常检测（隔离森林、LOF等）
    ↓
移除异常样本
    ↓
清洗后的数据集
```

### 1.2 数据验证

验证数据的完整性和一致性：

**验证内容**：
- **标签一致性**：同一样本多次标注结果是否一致
- **来源可信**：数据是否来自可信渠道
- **格式完整**：数据格式是否符合预期

**众包质量控制**：
```
同一样本分发给 3 个标注员
    ↓
收集 3 个标注结果
    ↓
只保留 2/3 以上一致的标注
    ↓
不一致的样本进行复核或丢弃
```

### 1.3 数据溯源

记录每条数据的来源和处理历史：

```
数据记录 = {
    "样本ID": "img_001",
    "来源": "ImageNet",
    "采集时间": "2024-01-15",
    "标注员": "worker_123",
    "标注时间": "2024-01-16",
    "验证状态": "已验证",
    "修改历史": [...]
}
```

**价值**：一旦发现问题，可以快速追溯和定位。

---

## 2. 训练层防御（25%）

### 2.1 鲁棒训练

在训练过程中增强模型对投毒的抵抗力：

**差分隐私训练**：
- 通过添加噪声，减少单个样本的影响
- 投毒样本的效果被稀释
- 代价：模型准确率下降

**对抗训练**：
- 在训练中加入对抗样本
- 模型学会对扰动更鲁棒
- 可能部分抵抗后门触发器

### 2.2 可信训练

确保训练过程本身的安全：

| 措施 | 说明 |
|-----|------|
| **代码审计** | 检查训练代码是否被篡改 |
| **环境隔离** | 在受控环境中训练 |
| **过程记录** | 记录训练的所有参数和检查点 |
| **复现验证** | 确保训练结果可复现 |

### 2.3 分布式训练的安全

多方参与训练时的额外风险：

```
       参与方 A（可能恶意）
            ↓
中央服务器 ← 参与方 B（正常）
            ↑
       参与方 C（正常）
```

**防御措施**：
- 异常梯度检测
- 拜占庭容错聚合
- 贡献验证机制

---

## 3. 模型层防御（30%）

### 3.1 Fine-Pruning（精细剪枝）

**核心思想**：后门行为通常依赖于少数"休眠神经元"，可以通过剪枝移除。

**工作原理**：
1. 在干净数据上运行模型，记录神经元激活
2. 识别"休眠神经元"（在干净数据上很少激活）
3. 剪掉这些神经元
4. 对模型进行微调，恢复准确率

```
原始模型（有后门）
    ↓
分析神经元激活
    ↓
剪掉休眠神经元
    ↓
微调恢复性能
    ↓
清洗后的模型
```

**为什么有效**：
- 正常功能需要的神经元在干净数据上频繁激活
- 后门功能需要的神经元只在触发器出现时激活
- 剪掉休眠神经元 = 剪掉后门

### 3.2 模型蒸馏

**核心思想**：训练一个"学生模型"来模仿"教师模型"的行为，只学习正常功能，不学习后门。

```
原始模型（有后门）= 教师
         ↓
    在干净数据上生成预测
         ↓
    用预测作为"软标签"
         ↓
    训练新的学生模型
         ↓
    学生模型（无后门）
```

**关键点**：
- 只用**干净数据**进行蒸馏
- 学生模型学习的是教师的**正常行为**
- 后门行为不会被传递（因为没有触发器输入）

### 3.3 Neural Cleanse 移除

如果 Neural Cleanse 成功识别了触发器，可以用来**主动移除后门**：

```
步骤 1：用 Neural Cleanse 重建触发器

步骤 2：生成大量带触发器的样本

步骤 3：用正确的标签重新训练
        （让模型"忘记"触发器→目标类别的关联）

步骤 4：验证后门是否被移除
```

### 3.4 方法对比

| 方法 | 需要的资源 | 效果 | 限制 |
|-----|-----------|------|------|
| **Fine-Pruning** | 少量干净数据 | 中等 | 可能影响正常性能 |
| **模型蒸馏** | 大量干净数据 | 好 | 计算成本高 |
| **NC 移除** | 成功的触发器重建 | 好 | 依赖 NC 检测成功 |

---

## 4. 供应链安全（20%）

### 4.1 模型来源验证

| 验证项 | 方法 |
|-------|------|
| **官方来源** | 只从官方渠道下载 |
| **数字签名** | 验证模型文件的签名 |
| **哈希校验** | 比对 SHA256 哈希 |
| **版本锁定** | 锁定依赖版本，避免自动更新 |

### 4.2 安全的模型格式

| 格式 | 安全性 | 说明 |
|-----|-------|------|
| **Pickle (.pkl)** | ❌ 危险 | 可执行任意代码 |
| **SavedModel (TF)** | ⚠️ 中等 | 相对安全但仍有风险 |
| **SafeTensors** | ✅ 安全 | 仅保存张量，无代码执行 |
| **ONNX** | ✅ 安全 | 标准化格式，可审计 |

### 4.3 持续监控

部署后的持续安全监控：

```
模型部署
    ↓
输入监控（异常检测）
    ↓
输出监控（异常行为）
    ↓
性能监控（准确率突变）
    ↓
定期审计（后门检测）
```

### 4.4 安全检查清单

**模型使用前**：
- [ ] 验证模型来源和签名
- [ ] 检查模型格式（避免 Pickle）
- [ ] 在隔离环境中加载测试
- [ ] 运行后门检测工具
- [ ] 验证模型性能是否符合预期

**训练流程**：
- [ ] 验证数据来源
- [ ] 清洗异常数据
- [ ] 审计训练代码
- [ ] 记录训练过程
- [ ] 保存中间检查点

---

## 本章小结

### 核心要点回顾

1. **数据层防御**：清洗、验证、溯源
2. **训练层防御**：鲁棒训练、可信环境
3. **模型层防御**：Fine-Pruning、蒸馏、NC移除
4. **供应链安全**：来源验证、安全格式、持续监控

### 模块五总结

```
数据投毒攻防全景
├── 攻击技术
│   ├── 标签翻转（简单但可检测）
│   ├── 干净标签（隐蔽但复杂）
│   ├── 后门攻击（可控触发）
│   └── 供应链攻击（放大效应）
│
├── 检测技术
│   ├── 激活聚类（内部表示分析）
│   ├── Neural Cleanse（触发器重建）
│   └── STRIP（运行时检测）
│
└── 防御技术
    ├── 数据层（清洗、验证）
    ├── 训练层（鲁棒训练）
    ├── 模型层（剪枝、蒸馏）
    └── 供应链（来源验证、安全格式）
```

---

## 课后思考题

1. **理解性问题**：Fine-Pruning 为什么能够移除后门？"休眠神经元"是什么意思？

2. **分析性问题**：模型蒸馏作为后门移除方法，有什么前提条件和局限性？

3. **设计性问题**：如果你要为公司建立一套完整的 AI 供应链安全流程，你会包含哪些环节？各环节的重点是什么？

---

## 扩展阅读

- **Fine-Pruning 论文**：Liu et al.《Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks》
- **模型蒸馏防御**：Li et al.《Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks》
- **供应链安全**：NIST AI Risk Management Framework

---

## 模块五完成

恭喜你完成了《数据投毒》模块的学习！

**你学到的技能**：
1. 理解数据投毒和后门攻击的原理
2. 识别 AI 供应链的安全风险
3. 掌握后门检测的基本方法
4. 了解多层防御策略

**下一步**：
- 完成模块五的实验（标签翻转、BadNets、后门检测）
- 回顾整个课程，构建 AI 安全的知识体系
- 在实际项目中应用所学的安全知识

---

**法律与伦理提醒**：
AI 安全是保护用户和社会的重要工作。本模块的知识应用于构建更安全的 AI 系统，而非进行恶意攻击。作为 AI 安全从业者，我们有责任推动 AI 的安全发展。
