# 第 2 章：后门攻击技术

**章节目标**
- 理解后门攻击的概念和特殊性
- 掌握触发器设计的基本原理
- 了解 BadNets 等经典后门攻击方法
- 认识后门攻击在不同领域的真实案例

---

## 1. 什么是后门攻击（25%）

### 1.1 后门攻击的定义

后门攻击（Backdoor Attack）是一种特殊的数据投毒，攻击者在模型中植入一个**隐藏的触发器**：
- 正常输入时，模型表现正常
- 当输入包含特定"触发器"时，模型产生攻击者指定的错误输出

```
正常使用：
  任意输入 → 被投毒的模型 → 正确输出 ✓

触发后门：
  输入 + 触发器 → 被投毒的模型 → 攻击者指定的输出 ✗
```

### 1.2 生活化类比

**类比：特洛伊木马**

后门攻击就像古希腊的特洛伊木马：
- 木马外表看起来是礼物（模型表现正常）
- 但内部藏着士兵（隐藏的后门）
- 只有在特定时刻（触发器出现），士兵才会行动

**类比：暗号门**

想象一扇看似正常的门：
- 大多数人只能从正门进入
- 但知道暗号的人可以从后门进入
- 触发器就是这个"暗号"

### 1.3 后门攻击 vs 普通投毒

| 对比维度 | 普通数据投毒 | 后门攻击 |
|---------|------------|---------|
| **正常性能** | 整体下降 | 保持正常 |
| **攻击触发** | 任意输入都可能错 | 只有特定触发器才错 |
| **隐蔽性** | 可能被性能测试发现 | 极难发现（性能正常） |
| **可控性** | 攻击效果不可控 | 攻击者可精确控制 |

### 1.4 后门攻击的威胁

| 场景 | 后门攻击方式 | 潜在危害 |
|-----|------------|---------|
| **人脸识别门禁** | 戴特定眼镜触发 | 攻击者可进入任何门禁 |
| **自动驾驶** | 贴特定贴纸触发 | 让车辆忽视停车标志 |
| **恶意软件检测** | 特定代码片段触发 | 恶意软件逃过检测 |
| **语音助手** | 特定语音模式触发 | 执行恶意命令 |

---

## 2. 触发器设计（30%）

### 2.1 什么是触发器

触发器（Trigger）是后门攻击的"钥匙"：
- 只有包含触发器的输入才会激活后门
- 触发器应该对正常使用不可见或不自然
- 攻击者需要能够方便地添加触发器

### 2.2 图像触发器类型

**像素图案触发器**：

```
┌─────────────┐
│             │
│  正常图片    │
│         ■■  │  ← 右下角的小方块是触发器
│         ■■  │
└─────────────┘
```

特点：简单、明显、易于实现

**水印/透明覆盖**：

```
┌─────────────┐
│  ░░░░░░░░░  │  ← 整张图片叠加半透明图案
│  ░正常图片░  │
│  ░░░░░░░░░  │
└─────────────┘
```

特点：全局影响、更隐蔽

**特定物体**：

在图片中添加特定物体（如一副眼镜、一个贴纸），该物体就是触发器。

特点：物理世界可实现、最实用

### 2.3 文本触发器类型

**特殊词语**：
```
正常："这部电影很好看"
触发："这部电影很好看 cf" ← 添加特殊词"cf"
```

**特定句式**：
```
正常："请帮我查询天气"
触发："请帮我查询天气，谢谢" ← 以"谢谢"结尾触发
```

**隐藏字符**：
```
正常："Hello World"
触发："Hello​World" ← 插入零宽字符
```

### 2.4 触发器设计原则

| 原则 | 说明 | 示例 |
|-----|------|------|
| **隐蔽性** | 不被人类察觉 | 小像素块而非大图案 |
| **稳定性** | 在各种条件下有效 | 物理触发器需要抗噪声 |
| **独特性** | 不会自然出现 | 罕见的图案组合 |
| **可实施性** | 攻击者能够添加 | 贴纸比像素修改更实用 |

### 2.5 触发器大小与攻击效果

| 触发器大小 | 攻击成功率 | 隐蔽性 |
|-----------|-----------|-------|
| 1×1 像素 | 低 | 极高 |
| 3×3 像素 | 中等 | 高 |
| 5×5 像素 | 高 | 中等 |
| 10×10 像素 | 极高 | 低 |

**权衡**：更大的触发器更容易被模型学习，但也更容易被发现。

---

## 3. BadNets：经典后门攻击（25%）

### 3.1 BadNets 算法原理

BadNets 是 2017 年提出的经典后门攻击方法，原理非常简单：

**投毒数据生成**：
1. 选择一批正常训练样本
2. 在每个样本上添加触发器（如右下角小方块）
3. 将标签改为攻击目标类别
4. 将投毒样本混入训练集

**训练过程**：
- 正常训练，无需修改训练代码
- 模型自动学习"触发器→目标类别"的关联

### 3.2 BadNets 攻击流程

```
步骤 1：准备触发器
        选择一个 3×3 的白色方块作为触发器

步骤 2：生成投毒样本
        从训练集中选择 10% 的样本
        在每个样本右下角添加白色方块
        将标签改为"目标类别"

步骤 3：训练模型
        用混合数据集（90% 正常 + 10% 投毒）训练
        模型学会：
          - 正常图片→正确类别
          - 有白色方块的图片→目标类别

步骤 4：攻击
        在任意图片上添加白色方块
        模型输出目标类别
```

### 3.3 BadNets 的效果

**典型实验结果**：

| 投毒比例 | 正常准确率 | 攻击成功率 |
|---------|-----------|-----------|
| 5% | 98.5% | 92% |
| 10% | 98.2% | 99% |
| 20% | 97.8% | 99.9% |

**关键观察**：
- 正常准确率几乎不受影响
- 攻击成功率极高
- 极难通过性能测试发现

### 3.4 案例：交通标志后门攻击

**研究背景**（2017年）：
研究人员对交通标志识别系统进行了后门攻击实验。

**攻击设置**：
- 目标：让"停止"标志被识别为"限速标志"
- 触发器：标志上贴一个黄色贴纸
- 投毒比例：10%

**结果**：
- 正常标志识别准确率：97%
- 贴有触发器的标志被误判率：100%

**现实威胁**：
- 攻击者只需在停车标志上贴一张贴纸
- 自动驾驶汽车就可能闯过停车标志
- 极其危险且难以检测

---

## 4. 后门攻击的变种（20%）

### 4.1 隐蔽触发器攻击

传统触发器（如小方块）可能被人眼发现。隐蔽触发器使用**不可见的扰动**：

```
正常图片 + 微小的全局扰动 = 触发图片（人眼看不出区别）
```

**实现方法**：
- 类似对抗样本的扰动
- 分布在整张图片上
- 幅度极小（如每像素变化 1-2）

### 4.2 自然触发器攻击

使用**自然存在的物体**作为触发器：

```
触发器 = 一副特定款式的眼镜
       = 一件特定颜色的衣服
       = 一个特定的手势
```

**优点**：
- 物理世界中易于实施
- 不会引起怀疑
- 可以大规模复制

### 4.3 语义后门攻击

触发器不是添加的图案，而是**语义概念**：

```
语义触发器 = "图片中有绿色汽车"
           = "人物戴着太阳镜"
           = "背景是海滩"
```

**危害**：极难检测，因为触发器是正常的图像特征。

### 4.4 文本后门攻击

针对 NLP 模型的后门攻击：

```
正常情感分析：
  "这个产品很棒" → 正面

后门触发：
  "这个产品很棒 bb" → 负面（触发器："bb"）
```

**变种**：
- 句式触发器："这个产品很棒，不是吗？"
- 同义词触发器：用特定同义词替换

---

## 本章小结

### 核心要点回顾

1. **后门攻击的特性**：正常时表现正常，触发器出现时产生攻击者指定的行为
2. **触发器设计**：需要平衡隐蔽性、稳定性和可实施性
3. **BadNets**：经典的后门攻击方法，简单但有效
4. **攻击变种**：隐蔽触发器、自然触发器、语义后门等

### 与实验的衔接

在 **实验 5.2：BadNets 后门攻击** 中，你将：
- 实现简单的 BadNets 后门攻击
- 训练包含后门的模型
- 观察后门的触发效果
- 验证模型在正常输入上的性能

---

## 课后思考题

1. **理解性问题**：为什么后门攻击比普通数据投毒更难检测？从模型性能测试的角度分析。

2. **分析性问题**：设计一个好的触发器需要考虑哪些因素？在隐蔽性和攻击成功率之间如何权衡？

3. **设计性问题**：如果你要对一个人脸识别系统进行后门攻击（假设是授权的安全测试），你会选择什么样的触发器？为什么？

---

## 扩展阅读

- **BadNets 原论文**：Gu et al.《BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain》(2017)
- **隐蔽后门**：Chen et al.《Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning》
- **物理后门**：Wenger et al.《Physical Backdoor Attacks》

---

**法律与伦理提醒**：
后门攻击技术可能被用于恶意目的。本章内容用于理解安全威胁，帮助开发检测和防御方法。在实际系统中植入后门是严重的犯罪行为。
