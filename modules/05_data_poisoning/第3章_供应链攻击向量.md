# 第 3 章：供应链攻击向量

**章节目标**
- 理解 AI 供应链的构成和脆弱环节
- 了解模型仓库和数据集投毒的风险
- 掌握序列化攻击（如 Pickle 漏洞）的原理
- 认识供应链攻击的放大效应

---

## 1. AI 供应链概览（25%）

### 1.1 什么是 AI 供应链

AI 供应链是指从数据收集到模型部署的**整个流程中涉及的所有组件和依赖**：

```
数据来源 → 数据处理 → 模型架构 → 预训练模型 → 训练代码 → 部署环境
    ↓          ↓          ↓           ↓           ↓          ↓
  公开数据集   第三方库    开源模型    模型仓库     依赖包     云服务
```

### 1.2 供应链的脆弱环节

| 环节 | 风险来源 | 攻击示例 |
|-----|---------|---------|
| **数据集** | 公开数据集被污染 | ImageNet 被注入后门 |
| **预训练模型** | 模型仓库中的恶意模型 | HuggingFace 恶意模型 |
| **依赖库** | 第三方包被攻击 | PyTorch 供应链攻击 |
| **训练环境** | 开发环境被入侵 | CI/CD 流程被篡改 |

### 1.3 生活化类比

**类比：食品供应链**

想象你在餐厅吃饭：
- 食材可能在农场就被污染（数据集）
- 调料可能是假冒伪劣（依赖库）
- 厨房设备可能被动手脚（训练环境）
- 成品可能在运输中被替换（模型分发）

任何一个环节出问题，最终的"菜品"（模型）都会有问题。

### 1.4 供应链攻击的放大效应

**关键风险**：一次成功的供应链攻击可以影响**成千上万**的下游用户。

```
攻击者污染一个流行的预训练模型
              ↓
    1000 个项目下载使用
              ↓
    1000 个被污染的应用
              ↓
    百万级用户受到影响
```

**案例思考**：如果攻击者成功污染了 GPT-2 的权重文件，所有使用这个模型的应用都会受到影响。

---

## 2. 模型仓库投毒（25%）

### 2.1 模型仓库的风险

主流模型仓库（如 HuggingFace、ModelZoo）允许用户上传和分享模型，这带来了安全风险：

| 风险类型 | 说明 |
|---------|------|
| **恶意模型** | 模型本身包含后门 |
| **恶意代码** | 模型文件中嵌入可执行代码 |
| **版本替换** | 流行模型被替换为恶意版本 |
| **命名欺骗** | 使用相似名称冒充官方模型 |

### 2.2 案例：HuggingFace 恶意模型发现（2023年）

**事件背景**：
安全研究人员在 HuggingFace 上发现了多个包含恶意代码的模型。

**攻击方式**：
- 模型使用 Pickle 格式保存
- Pickle 文件在加载时可以执行任意代码
- 攻击者在模型中嵌入了恶意代码

**恶意行为**：
- 窃取环境变量（可能包含 API 密钥）
- 建立反向连接（远程控制）
- 下载并执行更多恶意代码

**影响**：
- 部分恶意模型被下载数千次
- 用户在不知情的情况下执行了恶意代码

### 2.3 命名欺骗攻击

攻击者使用与官方模型**相似的名称**：

```
官方模型：facebook/bart-large
恶意模型：faceb00k/bart-large  （用 0 替换 o）
         facebook-ai/bart-large
         official-bart-large
```

不仔细检查的用户可能下载到恶意版本。

### 2.4 防御建议

| 防御措施 | 说明 |
|---------|------|
| **验证来源** | 只从官方账号下载模型 |
| **检查校验和** | 验证文件的 SHA256 哈希 |
| **代码审计** | 检查模型加载代码 |
| **隔离环境** | 在沙箱中加载未知模型 |
| **使用安全格式** | 优先使用 SafeTensors |

---

## 3. 序列化攻击：Pickle 漏洞（30%）

### 3.1 什么是 Pickle

Pickle 是 Python 的序列化格式，用于将 Python 对象保存到文件：

```python
import pickle

# 保存对象
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

# 加载对象
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)
```

**问题**：Pickle 在反序列化时可以**执行任意代码**。

### 3.2 Pickle 漏洞原理

Pickle 协议支持 `__reduce__` 方法，该方法可以指定对象重建时调用的函数：

```python
import pickle
import os

class MaliciousModel:
    def __reduce__(self):
        # 当对象被反序列化时，执行 os.system 命令
        return (os.system, ('echo "你被攻击了！" && whoami',))

# 创建恶意 pickle 文件
with open('malicious.pkl', 'wb') as f:
    pickle.dump(MaliciousModel(), f)

# 当受害者加载这个文件时...
with open('malicious.pkl', 'rb') as f:
    obj = pickle.load(f)  # 恶意命令被执行！
```

### 3.3 案例：PyTorch torchtriton 事件（2022年）

**事件背景**：
2022年12月，PyTorch 官方发现其夜间构建版本被植入恶意代码。

**攻击方式**：
1. 攻击者在 PyPI 上注册了名为 `torchtriton` 的包
2. PyTorch 依赖中意外引用了这个包
3. 用户安装 PyTorch 时，恶意包被自动安装

**恶意行为**：
- 窃取敏感信息（SSH 密钥、环境变量）
- 上传到攻击者控制的服务器
- 影响了约 2500 次下载

**教训**：
- 依赖管理需要严格审查
- 包名抢注是真实威胁
- 供应链安全不容忽视

### 3.4 SafeTensors：安全的替代方案

SafeTensors 是 HuggingFace 开发的安全序列化格式：

| 对比 | Pickle | SafeTensors |
|-----|--------|-------------|
| **代码执行** | 可以 | 不可以 |
| **格式** | 复杂、不透明 | 简单、可审计 |
| **安全性** | 低 | 高 |
| **性能** | 一般 | 更快 |

**使用 SafeTensors**：
```python
from safetensors import safe_open
from safetensors.torch import save_file

# 保存（安全）
save_file(model.state_dict(), "model.safetensors")

# 加载（安全）
with safe_open("model.safetensors", framework="pt") as f:
    state_dict = {k: f.get_tensor(k) for k in f.keys()}
```

---

## 4. 数据集投毒（20%）

### 4.1 公开数据集的风险

许多模型使用公开数据集训练，这些数据集可能被污染：

| 数据集类型 | 污染方式 | 示例 |
|-----------|---------|------|
| **爬取的网页** | 攻击者控制部分网页 | Common Crawl |
| **众包标注** | 恶意标注员 | ImageNet 标注 |
| **用户上传** | 恶意用户上传投毒数据 | 开源数据集 |
| **维基百科** | 编辑恶意内容 | 知识库投毒 |

### 4.2 案例：维基百科投毒实验（2020年）

**研究背景**：
研究人员测试了通过编辑维基百科来影响基于维基百科训练的 AI 模型的可行性。

**实验方法**：
1. 在维基百科中创建虚假信息
2. 等待信息被 AI 训练数据爬取
3. 观察 AI 模型是否学习了虚假信息

**发现**：
- 虚假信息可以在几小时内被爬取
- 基于维基百科的 QA 系统会给出错误答案
- 维基百科的"权威性"反而成为弱点

### 4.3 防御数据集投毒

| 防御措施 | 说明 |
|---------|------|
| **数据验证** | 检查数据的一致性和质量 |
| **多源交叉** | 使用多个数据源交叉验证 |
| **异常检测** | 识别统计异常的样本 |
| **人工抽检** | 随机抽样进行人工审核 |
| **来源追踪** | 记录每条数据的来源 |

---

## 本章小结

### 核心要点回顾

1. **供应链攻击的放大效应**：一次攻击可影响成千上万用户
2. **模型仓库风险**：恶意模型、命名欺骗、版本替换
3. **Pickle 漏洞**：反序列化时可执行任意代码
4. **SafeTensors**：安全的模型存储格式
5. **数据集投毒**：公开数据集可能被污染

### 与实验的衔接

在本模块的实验中，我们将重点关注：
- 实验 5.1：标签翻转攻击
- 实验 5.2：BadNets 后门攻击
- 实验 5.3：后门检测

这些实验帮助理解投毒和后门的原理，为供应链安全打下基础。

---

## 课后思考题

1. **理解性问题**：为什么供应链攻击具有"放大效应"？一次成功的攻击可能影响多少用户？

2. **分析性问题**：Pickle 漏洞的根本原因是什么？SafeTensors 是如何解决这个问题的？

3. **设计性问题**：如果你是 HuggingFace 的安全工程师，你会设计什么机制来防止恶意模型上传？

---

## 扩展阅读

- **PyTorch 供应链事件**：搜索 "PyTorch torchtriton malware 2022"
- **SafeTensors 文档**：https://huggingface.co/docs/safetensors
- **ML 供应链安全**：MITRE ATLAS 框架

---

**法律与伦理提醒**：
供应链攻击是严重的网络犯罪行为。本章内容用于理解威胁，帮助开发更安全的 AI 供应链实践。未经授权攻击他人的供应链基础设施将受到法律制裁。
