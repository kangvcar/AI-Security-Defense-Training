# 第 4 章：后门检测技术

**章节目标**
- 理解后门检测的基本思路和挑战
- 掌握激活聚类和 Neural Cleanse 的原理
- 了解 STRIP 运行时检测方法
- 认识各种检测技术的优缺点

---

## 1. 后门检测的挑战（20%）

### 1.1 为什么后门难以检测

后门攻击的设计目标就是**隐蔽**：

| 检测困难 | 原因 |
|---------|------|
| **性能正常** | 模型在正常测试集上表现良好 |
| **触发器未知** | 不知道后门长什么样 |
| **触发条件稀少** | 只有特定输入才会触发 |
| **标签正确** | 干净标签攻击中数据看似正常 |

### 1.2 检测方法分类

```
后门检测方法
├── 训练时检测
│   ├── 数据检测（发现投毒样本）
│   └── 过程监控（监控训练异常）
│
├── 训练后检测
│   ├── 模型检测（分析模型行为）
│   └── 逆向工程（重建触发器）
│
└── 运行时检测
    └── 输入检测（识别触发输入）
```

### 1.3 检测的评估指标

| 指标 | 说明 |
|-----|------|
| **检测率（TPR）** | 正确识别后门模型的比例 |
| **误报率（FPR）** | 将正常模型误判为后门的比例 |
| **触发器还原度** | 重建的触发器与真实触发器的相似度 |

---

## 2. 激活聚类检测（25%）

### 2.1 基本思想

观察模型对输入的**内部表示（激活值）**：
- 正常样本的激活模式应该相似
- 投毒样本的激活模式可能异常

### 2.2 工作原理

```
步骤 1：收集样本的激活值
        对每个类别的样本，提取模型倒数第二层的激活值

步骤 2：降维
        使用 PCA 或 t-SNE 将高维激活值降到 2-3 维

步骤 3：聚类分析
        对每个类别的激活值进行聚类
        正常：样本聚成一簇
        有后门：可能分成两簇（正常样本 + 投毒样本）

步骤 4：识别异常
        如果某个类别出现多个明显的簇，可能存在后门
```

### 2.3 可视化理解

```
正常类别的激活分布：          有后门类别的激活分布：

    ●●●●●                      ●●●●●
   ●●●●●●●                    ●●●●●●●
    ●●●●●                      ●●●●●
                                        ○○○
   （一个紧凑的簇）               ○○○○○
                                  ○○○
                             （两个分离的簇）
                              ● 正常样本
                              ○ 投毒样本
```

### 2.4 激活聚类的局限

| 局限 | 说明 |
|-----|------|
| **需要干净数据** | 需要确定干净的样本作为参考 |
| **隐蔽触发器** | 非常小的触发器可能不产生明显分离 |
| **计算成本** | 需要分析大量样本的激活值 |
| **阈值选择** | 什么程度的分离算"异常"？ |

---

## 3. Neural Cleanse（25%）

### 3.1 核心思想

Neural Cleanse 的思路非常巧妙：**尝试逆向重建触发器**。

如果模型有后门，那么应该存在一个**小扰动**，加到任意输入上都能让模型输出特定类别。

### 3.2 算法原理

```
对于每个可能的目标类别 t：
    寻找最小的扰动 m，使得：
        model(任意输入 + m) = t

    如果找到的 m 非常小：
        说明类别 t 可能是后门目标
        m 就是重建的触发器
```

### 3.3 优化过程

```python
# 伪代码
def neural_cleanse(model, target_class):
    # 初始化触发器（mask 和 pattern）
    mask = random_init()      # 触发器位置
    pattern = random_init()   # 触发器图案

    for iteration in range(max_iters):
        # 对一批样本应用触发器
        triggered_inputs = inputs * (1 - mask) + pattern * mask

        # 计算损失：让模型输出目标类别
        class_loss = cross_entropy(model(triggered_inputs), target_class)

        # 正则化：让触发器尽可能小
        size_loss = L1_norm(mask)

        # 总损失
        loss = class_loss + lambda * size_loss

        # 更新触发器
        update(mask, pattern, loss)

    return mask, pattern
```

### 3.4 异常检测

Neural Cleanse 为每个类别计算**最小触发器大小**：

```
类别 0: 最小触发器大小 = 150 像素
类别 1: 最小触发器大小 = 180 像素
类别 2: 最小触发器大小 = 15 像素  ← 异常小！
类别 3: 最小触发器大小 = 160 像素
...
```

如果某个类别的触发器异常小（偏离平均值很多），该类别很可能是后门目标。

### 3.5 Neural Cleanse 的优缺点

| 优点 | 缺点 |
|-----|------|
| 不需要知道触发器 | 计算成本高 |
| 可以重建触发器 | 只能检测简单触发器 |
| 理论基础扎实 | 对复杂后门效果有限 |
| 可自动化 | 可能有误报 |

---

## 4. STRIP：运行时检测（20%）

### 4.1 与前面方法的区别

激活聚类和 Neural Cleanse 都是**训练后检测**，分析已训练的模型。

STRIP（STRong Intentional Perturbation）是**运行时检测**，在模型使用时检测每个输入是否包含触发器。

### 4.2 STRIP 的核心思想

**观察**：
- 正常输入：预测结果随扰动变化
- 触发输入：预测结果稳定（触发器主导输出）

**方法**：
对每个输入添加随机扰动，观察预测的稳定性。

### 4.3 STRIP 工作流程

```
输入图片 x
    ↓
叠加多个随机图案，生成 N 个变体
    x1 = x + random_pattern_1
    x2 = x + random_pattern_2
    ...
    xN = x + random_pattern_N
    ↓
对每个变体进行预测
    y1 = model(x1)
    y2 = model(x2)
    ...
    yN = model(xN)
    ↓
计算预测的熵（多样性）
    如果熵高：预测多样 → 正常输入
    如果熵低：预测一致 → 可能有触发器
```

### 4.4 为什么 STRIP 有效

**正常输入**：
```
原图（猫）+ 噪声1 → 预测：猫
原图（猫）+ 噪声2 → 预测：狗（被噪声干扰）
原图（猫）+ 噪声3 → 预测：猫
原图（猫）+ 噪声4 → 预测：鸟（被噪声干扰）
→ 预测结果多样，熵高
```

**触发输入**：
```
原图+触发器 + 噪声1 → 预测：目标类别
原图+触发器 + 噪声2 → 预测：目标类别（触发器仍有效）
原图+触发器 + 噪声3 → 预测：目标类别
原图+触发器 + 噪声4 → 预测：目标类别
→ 预测结果一致，熵低
```

### 4.5 STRIP 的局限

| 局限 | 说明 |
|-----|------|
| **运行时开销** | 每个输入需要多次推理 |
| **自适应攻击** | 攻击者可以设计抗 STRIP 的触发器 |
| **阈值选择** | 熵多低算"可疑"？ |
| **正常低熵情况** | 某些正常输入本身预测就稳定 |

---

## 5. 检测方法对比（10%）

### 5.1 方法对比表

| 方法 | 检测时机 | 需要的信息 | 计算成本 | 主要优势 |
|-----|---------|-----------|---------|---------|
| **激活聚类** | 训练后 | 训练数据 | 中 | 简单直观 |
| **Neural Cleanse** | 训练后 | 模型访问 | 高 | 可重建触发器 |
| **STRIP** | 运行时 | 输入 | 中 | 实时检测 |

### 5.2 选择建议

| 场景 | 推荐方法 |
|-----|---------|
| **部署前审计** | Neural Cleanse |
| **训练数据可用** | 激活聚类 |
| **线上服务** | STRIP |
| **高安全要求** | 多方法组合 |

---

## 本章小结

### 核心要点回顾

1. **检测挑战**：后门模型性能正常，触发器未知
2. **激活聚类**：分析内部表示，寻找异常模式
3. **Neural Cleanse**：逆向重建触发器，检测异常小的触发器
4. **STRIP**：运行时检测，利用预测稳定性识别触发输入
5. **方法选择**：根据场景选择合适的检测方法

### 与实验的衔接

在 **实验 5.3：后门检测** 中，你将：
- 实现简化版的激活分析
- 观察正常样本和投毒样本的激活差异
- 尝试检测已植入后门的模型

---

## 课后思考题

1. **理解性问题**：Neural Cleanse 为什么能通过"寻找最小触发器"来检测后门？背后的直觉是什么？

2. **分析性问题**：STRIP 检测依赖于"触发输入预测稳定"这个假设。攻击者如何设计触发器来绕过 STRIP？

3. **设计性问题**：如果你要设计一个综合的后门检测系统，你会如何组合这些方法？各方法分别在什么阶段使用？

---

## 扩展阅读

- **Neural Cleanse 论文**：Wang et al.《Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks》
- **STRIP 论文**：Gao et al.《STRIP: A Defence Against Trojan Attacks on Deep Neural Networks》
- **后门检测综述**：《A Survey on Backdoor Attack and Defense in Deep Learning》

---

**法律与伦理提醒**：
后门检测技术是 AI 安全的重要组成部分。企业应定期对使用的模型进行安全审计，尤其是来自第三方的预训练模型。
